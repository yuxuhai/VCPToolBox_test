// VectorDBManager.js
const { Worker } = require('worker_threads');
const fs = require('fs').promises;
const path = require('path');
const chokidar = require('chokidar');
const { HierarchicalNSW } = require('hnswlib-node');
const crypto = require('crypto');
const { chunkText } = require('./TextChunker.js');
const WorkerPool = require('./WorkerPool.js');
const VectorDBStorage = require('./VectorDBStorage.js');
const TagVectorManager = require('./TagVectorManager.js');
const TagExpander = require('./TagExpander.js');

// --- Constants ---
const DIARY_ROOT_PATH = path.join(__dirname, 'dailynote'); // Your diary root directory
const VECTOR_STORE_PATH = path.join(__dirname, 'VectorStore'); // Directory to store vector indices

/**
 * LRU Cache with TTL for search results
 */
class SearchCache {
    constructor(maxSize = 100, ttl = 60000) { // 1-minute TTL
        this.cache = new Map();
        this.maxSize = maxSize;
        this.ttl = ttl;
        this.hits = 0;
        this.misses = 0;
    }

    getCacheKey(diaryName, queryVector, k) {
        const vectorHash = crypto.createHash('md5')
            .update(JSON.stringify(queryVector))
            .digest('hex');
        return `${diaryName}-${vectorHash}-${k}`;
    }

    get(diaryName, queryVector, k) {
        const key = this.getCacheKey(diaryName, queryVector, k);
        const entry = this.cache.get(key);
        
        if (entry && Date.now() - entry.timestamp < this.ttl) {
            this.hits++;
            return entry.result;
        }
        
        this.cache.delete(key);
        this.misses++;
        return null;
    }

    set(diaryName, queryVector, k, result) {
        const key = this.getCacheKey(diaryName, queryVector, k);
        
        if (this.cache.size >= this.maxSize) {
            const firstKey = this.cache.keys().next().value;
            this.cache.delete(firstKey);
        }
        
        this.cache.set(key, {
            result,
            timestamp: Date.now()
        });
    }

    getStats() {
        const total = this.hits + this.misses;
        return {
            hits: this.hits,
            misses: this.misses,
            hitRate: total > 0 ? (this.hits / total * 100).toFixed(2) + '%' : '0%',
            size: this.cache.size,
            maxSize: this.maxSize
        };
    }
}

/**
 * Manages the creation, synchronization, and searching of vector databases for diaries.
 */
class VectorDBManager {
    constructor(config = {}) {
        this.config = {
            changeThreshold: parseFloat(process.env.VECTORDB_CHANGE_THRESHOLD) || 0.5,
            maxMemoryUsage: (parseInt(process.env.VECTORDB_MAX_MEMORY_MB) || 500) * 1024 * 1024,
            cacheSize: parseInt(process.env.VECTORDB_CACHE_SIZE) || 100,
            cacheTTL: parseInt(process.env.VECTORDB_CACHE_TTL_MS) || 60000,
            retryAttempts: parseInt(process.env.VECTORDB_RETRY_ATTEMPTS) || 3,
            retryBaseDelay: parseInt(process.env.VECTORDB_RETRY_BASE_DELAY_MS) || 1000,
            retryMaxDelay: parseInt(process.env.VECTORDB_RETRY_MAX_DELAY_MS) || 10000,
            preWarmCount: parseInt(process.env.VECTORDB_PREWARM_COUNT) || 5,
            efSearch: parseInt(process.env.VECTORDB_EF_SEARCH) || 150,
            rateLimitPauseMs: parseInt(process.env.VECTORDB_RATE_LIMIT_PAUSE_MS) || 120000, // é»˜è®¤2åˆ†é’Ÿ
            debug: process.env.VECTORDB_DEBUG === 'true',
        };

        this.apiKey = process.env.API_Key;
        this.apiUrl = process.env.API_URL;
        this.embeddingModel = process.env.WhitelistEmbeddingModel;

        // âœ… æœŸæœ›çš„embeddingç»´åº¦ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå¦‚æœè®¾ç½®åˆ™å¼ºåˆ¶éªŒè¯ï¼‰
        this.expectedDimensions = process.env.VECTORDB_DIMENSION ? parseInt(process.env.VECTORDB_DIMENSION) : null;
        
        // âœ… ç¼“å­˜embeddingç»´åº¦ï¼ˆåˆå§‹åŒ–æ—¶æ¢æµ‹ä¸€æ¬¡ï¼‰
        this.embeddingDimensions = null;

        this.indices = new Map();
        this.chunkMaps = new Map();
        this.activeWorkers = new Set();
        this.lruCache = new Map();
        this.searchCache = new SearchCache(this.config.cacheSize, this.config.cacheTTL);
        this.searchWorkerPool = new WorkerPool(path.resolve(__dirname, 'vectorSearchWorker.js'));
        this.fileLocks = new Map();
        
        // âœ… SQLiteå­˜å‚¨å±‚
        this.storage = new VectorDBStorage(VECTOR_STORE_PATH);
        
        // âœ… Tagå‘é‡ç®¡ç†å™¨
        this.tagVectorManager = null;
        this.tagVectorEnabled = false;
        this.tagRAGSystemEnabled = process.env.tagRAGSystem === 'true'; // ğŸŒŸ Tag RAGç³»ç»Ÿæ€»å¼€å…³
        
        // ğŸŒŸ Tagæ‰©å±•å™¨ï¼ˆæ¯›è¾¹ç½‘ç»œï¼‰
        this.tagExpander = null;
        this.tagExpanderEnabled = false;
        
        // âœ… æ‰¹é‡å†™å…¥ä¼˜åŒ–
        this.usageStatsBuffer = new Map();
        this.usageStatsFlushTimer = null;
        this.usageStatsFlushDelay = 5000; // 5ç§’é˜²æŠ–
        this.isShuttingDown = false;

        this.stats = {
            totalIndices: 0,
            totalChunks: 0,
            totalSearches: 0,
            avgSearchTime: 0,
            lastUpdateTime: null,
        };

        console.log('[VectorDB] Initialized with config:', {
            changeThreshold: this.config.changeThreshold,
            maxMemoryMB: this.config.maxMemoryUsage / 1024 / 1024,
            cacheSize: this.config.cacheSize,
            cacheTTL: this.config.cacheTTL,
            retryAttempts: this.config.retryAttempts,
        });
    }

    debugLog(message, ...args) {
        if (this.config.debug) {
            console.log(`[VectorDB][DEBUG] ${message}`, ...args);
        }
    }

    /**
     * è·å–æ–‡ä»¶é”ï¼ˆå¸¦è¶…æ—¶ï¼‰
     * @param {string} diaryName - æ—¥è®°æœ¬åç§°
     */
    async acquireLock(diaryName) {
        const lockKey = `lock_${diaryName}`;
        let attempts = 0;
        const maxAttempts = 100; // 5ç§’è¶…æ—¶ï¼ˆ100 * 50msï¼‰
        
        while (this.fileLocks.get(lockKey)) {
            if (attempts++ >= maxAttempts) {
                const lock = this.fileLocks.get(lockKey);
                const heldDuration = Date.now() - (lock?.acquiredAt || 0);
                throw new Error(
                    `[VectorDB] Failed to acquire lock for "${diaryName}" after ${maxAttempts * 50}ms.\n` +
                    `Lock acquired at: ${new Date(lock?.acquiredAt).toISOString()}\n` +
                    `Held for: ${heldDuration}ms`
                );
            }
            await new Promise(resolve => setTimeout(resolve, 50));
        }
        
        this.fileLocks.set(lockKey, {
            acquiredAt: Date.now(),
            stack: new Error().stack // âœ… è°ƒè¯•ç”¨ï¼šè®°å½•è°ƒç”¨æ ˆ
        });
        
        this.debugLog(`Lock acquired for "${diaryName}"`);
    }

    /**
     * é‡Šæ”¾æ–‡ä»¶é”
     * @param {string} diaryName - æ—¥è®°æœ¬åç§°
     */
    releaseLock(diaryName) {
        const lockKey = `lock_${diaryName}`;
        const lock = this.fileLocks.get(lockKey);
        
        if (lock) {
            const heldDuration = Date.now() - lock.acquiredAt;
            this.fileLocks.delete(lockKey);
            this.debugLog(`Lock released for "${diaryName}" (held for ${heldDuration}ms)`);
        } else {
            console.warn(`[VectorDB] Attempted to release non-existent lock for "${diaryName}"`);
        }
    }

    /**
     * è®°å½•æ€§èƒ½æŒ‡æ ‡
     */
    recordMetric(type, duration) {
        if (type === 'search_success') {
            this.stats.totalSearches++;
            this.stats.avgSearchTime =
                (this.stats.avgSearchTime * (this.stats.totalSearches - 1) + duration)
                / this.stats.totalSearches;
        }
    }

    getHealthStatus() {
        const totalChunks = Array.from(this.chunkMaps.values()).reduce((sum, map) => sum + Object.keys(map).length, 0);
        const healthStatus = {
            status: 'healthy',
            stats: {
                ...this.stats,
                totalIndices: this.indices.size,
                totalChunks: totalChunks,
                workerQueueLength: this.activeWorkers.size,
                memoryUsage: process.memoryUsage().heapUsed,
            },
            activeWorkers: Array.from(this.activeWorkers),
            loadedIndices: Array.from(this.indices.keys()),
            dbStats: this.storage.getStats(),
            cacheStats: this.searchCache.getStats(),
        };
        
        // âœ… æ·»åŠ Tagå‘é‡ç»Ÿè®¡
        if (this.tagVectorEnabled && this.tagVectorManager) {
            healthStatus.tagStats = this.tagVectorManager.getStats();
        }
        
        // ğŸŒŸ æ·»åŠ Tagæ‰©å±•å™¨ç»Ÿè®¡
        if (this.tagExpanderEnabled && this.tagExpander) {
            healthStatus.tagExpanderStats = this.tagExpander.getStats();
        }
        
        return healthStatus;
    }

    async initialize() {
        console.log('[VectorDB] Initializing Vector Database Manager...');
        await fs.mkdir(VECTOR_STORE_PATH, { recursive: true });
        await this.storage.initialize();
        
        // âœ… åˆå§‹åŒ–æ—¶æ¢æµ‹embeddingç»´åº¦ï¼ˆé‡‘æ ‡å‡†ï¼‰
        // ğŸŒŸ æ–°å¢ï¼šæ”¯æŒVECTORDB_DIMENSIONç¯å¢ƒå˜é‡è¿›è¡Œä¸¥æ ¼éªŒè¯
        try {
            const cachedDimensions = this.storage.getEmbeddingDimensions();
            
            // ğŸŒŸ å¦‚æœè®¾ç½®äº†æœŸæœ›ç»´åº¦ï¼Œè¿›è¡Œä¸¥æ ¼éªŒè¯
            if (this.expectedDimensions) {
                console.log(`[VectorDB] ğŸ” Expected dimensions from config: ${this.expectedDimensions}D`);
                
                // æ€»æ˜¯è¿›è¡ŒAPIæ¢é’ˆéªŒè¯ï¼ˆå³ä½¿æœ‰ç¼“å­˜ä¹Ÿè¦éªŒè¯ï¼‰
                console.log('[VectorDB] Probing API to verify embedding dimensions...');
                const dummyEmbeddings = await this.getEmbeddingsWithRetry(["."]);
                
                if (!dummyEmbeddings || dummyEmbeddings.length === 0) {
                    throw new Error('Failed to get embedding response from API');
                }
                
                const actualDimensions = dummyEmbeddings[0].length;
                console.log(`[VectorDB] ğŸ“Š API returned dimensions: ${actualDimensions}D`);
                
                // âš ï¸ ä¸¥æ ¼éªŒè¯ï¼šå®é™…ç»´åº¦å¿…é¡»åŒ¹é…æœŸæœ›ç»´åº¦
                if (actualDimensions !== this.expectedDimensions) {
                    const errorMsg = `
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âŒ EMBEDDING DIMENSION MISMATCH ERROR                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Expected: ${this.expectedDimensions}D (from VECTORDB_DIMENSION env var)         â•‘
â•‘  Actual:   ${actualDimensions}D (from API response)                     â•‘
â•‘                                                                â•‘
â•‘  ğŸ” Possible causes:                                           â•‘
â•‘  1. Wrong embedding model configured                           â•‘
â•‘  2. API endpoint doesn't support ${this.expectedDimensions}D model            â•‘
â•‘  3. Model mismatch (check WhitelistEmbeddingModel)             â•‘
â•‘                                                                â•‘
â•‘  ğŸ’¡ Solutions:                                                 â•‘
â•‘  1. Check your API_URL and WhitelistEmbeddingModel settings    â•‘
â•‘  2. Verify the model supports ${this.expectedDimensions}D embeddings           â•‘
â•‘  3. Update VECTORDB_DIMENSION to match your model (${actualDimensions}D)       â•‘
â•‘  4. Remove VECTORDB_DIMENSION to auto-detect                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
`;
                    console.error(errorMsg);
                    throw new Error(`Dimension mismatch: expected ${this.expectedDimensions}D but got ${actualDimensions}D from API`);
                }
                
                // âœ… éªŒè¯é€šè¿‡
                this.embeddingDimensions = actualDimensions;
                console.log(`[VectorDB] âœ… Dimension validation passed: ${this.embeddingDimensions}D`);
                
                // æ›´æ–°ç¼“å­˜ï¼ˆå¦‚æœä¸ç¼“å­˜ä¸åŒï¼‰
                if (cachedDimensions !== this.embeddingDimensions) {
                    this.storage.saveEmbeddingDimensions(this.embeddingDimensions);
                    console.log(`[VectorDB] ğŸ’¾ Updated cached dimensions: ${this.embeddingDimensions}D`);
                }
                
            } else {
                // ğŸ”„ ä¼ ç»Ÿæ¨¡å¼ï¼šè‡ªåŠ¨æ£€æµ‹ç»´åº¦ï¼ˆæ— å¼ºåˆ¶éªŒè¯ï¼‰
                if (cachedDimensions) {
                    this.embeddingDimensions = cachedDimensions;
                    console.log(`[VectorDB] âœ… Loaded cached embedding dimensions: ${this.embeddingDimensions}D`);
                } else {
                    console.log('[VectorDB] No cached dimensions found, probing API...');
                    const dummyEmbeddings = await this.getEmbeddingsWithRetry(["."]);
                    if (dummyEmbeddings && dummyEmbeddings.length > 0) {
                        this.embeddingDimensions = dummyEmbeddings[0].length;
                        // âœ… ä¿å­˜åˆ°æ•°æ®åº“ç¼“å­˜
                        this.storage.saveEmbeddingDimensions(this.embeddingDimensions);
                        console.log(`[VectorDB] âœ… Embedding dimensions detected and cached: ${this.embeddingDimensions}D`);
                    } else {
                        throw new Error('Failed to detect embedding dimensions');
                    }
                }
            }
        } catch (error) {
            console.error('[VectorDB] Failed to initialize embedding dimensions:', error.message);
            throw new Error(`Cannot initialize VectorDB: ${error.message}`);
        }
        
        // âœ… åˆå§‹åŒ–Tagå‘é‡ç®¡ç†å™¨ï¼ˆå¼‚æ­¥åå°ï¼Œä¸é˜»å¡å¯åŠ¨ï¼‰
        if (this.tagRAGSystemEnabled) {
            console.log('[VectorDB] Tag RAG System is ENABLED');
            this.initializeTagVectorManager(); // âš ï¸ ä¸ä½¿ç”¨ awaitï¼Œè®©å®ƒåœ¨åå°è¿è¡Œ
        } else {
            console.log('[VectorDB] Tag RAG System is DISABLED (tagRAGSystem=false)');
        }
        
        await this.scanAndSyncAll();
        await this.cacheDiaryNameVectors();
        await this.preWarmIndices();
        this.watchDiaries();
        console.log('[VectorDB] Initialization complete. Now monitoring diary files for changes.');
    }

    /**
     * âœ… åˆå§‹åŒ–Tagå‘é‡ç®¡ç†å™¨ï¼ˆå¼‚æ­¥åå°æ„å»ºï¼‰
     */
    async initializeTagVectorManager() {
        // ğŸŒŸ åŒé‡æ£€æŸ¥ï¼šå³ä½¿è¢«è°ƒç”¨ï¼Œä¹Ÿè¦æ£€æŸ¥å¼€å…³çŠ¶æ€
        if (!this.tagRAGSystemEnabled) {
            console.log('[VectorDB] Tag Vector Manager initialization skipped (system disabled)');
            return;
        }
        
        try {
            console.log('[VectorDB] Initializing Tag Vector Manager...');
            
            this.tagVectorManager = new TagVectorManager({
                diaryRootPath: DIARY_ROOT_PATH,
                vectorStorePath: VECTOR_STORE_PATH
            });
            
            // ä¼ å…¥embeddingå‡½æ•°
            const embeddingFunction = async (texts) => {
                return await this.getEmbeddingsWithRetry(texts);
            };
            
            // âœ… å¼‚æ­¥åˆå§‹åŒ–ï¼šä¸é˜»å¡æœåŠ¡å™¨å¯åŠ¨
            console.log('[VectorDB] Tag Vector Manager will build in background...');
            this.tagVectorManager.initialize(embeddingFunction).then(() => {
                this.tagVectorEnabled = true;
                const stats = this.tagVectorManager.getStats();
                console.log(`[VectorDB] âœ… Tag Vector Manager ready:`, {
                    totalTags: stats.totalTags,
                    vectorizedTags: stats.vectorizedTags,
                    blacklistedTags: stats.blacklistedTags
                });
                console.log('[VectorDB] ğŸ‰ Tag-based search is now available!');
                
                // ğŸŒŸ åˆå§‹åŒ–Tagæ‰©å±•å™¨ï¼ˆåœ¨Tagå‘é‡ç®¡ç†å™¨å°±ç»ªåï¼‰
                this.initializeTagExpander();
            }).catch(error => {
                console.error('[VectorDB] Tag Vector Manager build failed:', error);
                this.tagVectorEnabled = false;
            });
            
            // ç«‹å³è¿”å›ï¼Œä¸ç­‰å¾…æ„å»ºå®Œæˆ
            console.log('[VectorDB] Tag Vector Manager is building in background, server continues...');
            
        } catch (error) {
            console.error('[VectorDB] Failed to start Tag Vector Manager:', error);
            console.warn('[VectorDB] Tag-based search will be disabled');
            this.tagVectorEnabled = false;
        }
    }

    /**
     * ğŸŒŸ åˆå§‹åŒ–Tagæ‰©å±•å™¨ï¼ˆå¼‚æ­¥åå°ï¼‰
     */
    async initializeTagExpander() {
        try {
            console.log('[VectorDB] Initializing Tag Expander...');
            
            this.tagExpander = new TagExpander({
                debug: this.config.debug
            });
            
            // ğŸŒŸ ä»å…±ç°æ•°æ®åº“å¯¼å‡ºæƒé‡çŸ©é˜µå¹¶åŠ è½½åˆ°å†…å­˜
            if (this.tagVectorManager?.cooccurrenceDB) {
                const weightMatrix = this.tagVectorManager.cooccurrenceDB.exportWeightMatrix();
                await this.tagExpander.loadWeightMatrix(weightMatrix);
                this.tagExpanderEnabled = true;
                
                const expanderStats = this.tagExpander.getStats();
                console.log('[VectorDB] âœ… Tag Expander ready:', {
                    totalTags: expanderStats.totalTags,
                    totalEdges: expanderStats.totalEdges,
                    avgDegree: expanderStats.avgDegree
                });
                console.log('[VectorDB] ğŸ‰ Tag graph expansion is now available!');
            } else {
                console.warn('[VectorDB] Cooccurrence DB not available, Tag Expander disabled');
                this.tagExpanderEnabled = false;
            }
            
        } catch (error) {
            console.error('[VectorDB] Failed to initialize Tag Expander:', error);
            this.tagExpanderEnabled = false;
        }
    }

    async scanAndSyncAll() {
        console.log('[VectorDB] Scanning all diary books for updates...');
        
        try {
            const diaryBooks = await fs.readdir(DIARY_ROOT_PATH, { withFileTypes: true });
            
            // âœ… æ‰¹é‡å¤„ç†ï¼Œé¿å…å¹¶å‘é—®é¢˜
            const updateTasks = [];
            
            // âœ… æ–°å¢ï¼šæ”¶é›†å½“å‰å­˜åœ¨çš„æ—¥è®°æœ¬åç§°
            const currentDiaryNames = new Set();
            
            console.log(`[VectorDB] Found ${diaryBooks.length} items in diary root path`);
            
            for (const dirent of diaryBooks) {
                if (dirent.isDirectory()) {
                    const diaryName = dirent.name;
                    if (diaryName.startsWith('å·²æ•´ç†') || diaryName === 'VCPè®ºå›') {
                        this.debugLog(`Ignoring folder "${diaryName}" as it is in the exclusion list.`);
                        continue;
                    }
                    
                    currentDiaryNames.add(diaryName);
                    const diaryPath = path.join(DIARY_ROOT_PATH, diaryName);
                    
                    console.log(`[VectorDB] Checking if update needed for "${diaryName}"...`);
                    const needsUpdate = await this.checkIfUpdateNeeded(diaryName, diaryPath);
                    if (needsUpdate) {
                        console.log(`[VectorDB] Changes detected in "${diaryName}", will schedule update.`);
                        updateTasks.push(diaryName);
                    } else {
                        console.log(`[VectorDB] "${diaryName}" is up-to-date. Index will be loaded on demand.`);
                    }
                }
            }
            
            console.log(`[VectorDB] Finished checking all diaries. Found ${updateTasks.length} that need updates.`);
            
            // âœ… æ–°å¢ï¼šæ¸…ç†æ•°æ®åº“ä¸­å·²åˆ é™¤çš„æ—¥è®°æœ¬
            const dbDiaryNames = this.storage.getAllDiaryNames();
            console.log(`[VectorDB] Checking for orphaned database entries (${dbDiaryNames.length} in DB, ${currentDiaryNames.size} on disk)...`);
            
            for (const dbDiaryName of dbDiaryNames) {
                if (!currentDiaryNames.has(dbDiaryName)) {
                    console.log(`[VectorDB] Found orphaned database entry for deleted diary "${dbDiaryName}". Cleaning up...`);
                    await this.cleanupDeletedDiary(dbDiaryName);
                }
            }
            
            // âœ… ç»Ÿä¸€è°ƒåº¦æ›´æ–°ä»»åŠ¡ï¼ˆéé˜»å¡ï¼‰
            console.log(`[VectorDB] Scheduling ${updateTasks.length} diary books for update...`);
            for (const diaryName of updateTasks) {
                // âš ï¸ å…³é”®ä¿®å¤ï¼šä¸ç­‰å¾…è°ƒåº¦å®Œæˆï¼Œè®©å®ƒå¼‚æ­¥æ‰§è¡Œ
                this.scheduleDiaryBookProcessing(diaryName).catch(err => {
                    console.error(`[VectorDB] Failed to schedule processing for "${diaryName}":`, err);
                });
            }
            
            console.log(`[VectorDB] âœ… scanAndSyncAll completed successfully`);
        } catch (error) {
            console.error(`[VectorDB] âŒ scanAndSyncAll failed:`, error);
            throw error;
        }
    }

    async checkIfUpdateNeeded(diaryName, diaryPath) {
        console.log(`[VectorDB] Checking update needed for "${diaryName}"...`);
        
        // âœ… æ£€æŸ¥æ˜¯å¦åœ¨æš‚åœæœŸ
        if (this.storage.isRebuildPaused(diaryName)) {
            this.debugLog(`[VectorDB] Update check for "${diaryName}" is paused`);
            return false;
        }

        const safeFileNameBase = Buffer.from(diaryName, 'utf-8').toString('base64url');
        const indexPath = path.join(VECTOR_STORE_PATH, `${safeFileNameBase}.bin`);
        const indexExists = await this.fileExists(indexPath);
        
        console.log(`[VectorDB] Index exists for "${diaryName}": ${indexExists}`);
        
        // âœ… æ­¥éª¤1ï¼šæ£€æŸ¥æ•°æ®åº“æ˜¯å¦æœ‰æ•°æ®ï¼ˆä¼˜åŒ–ï¼šåªè·å–æ•°é‡ï¼Œä¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰
        const dbChunkCount = this.storage.getChunkCount(diaryName);
        console.log(`[VectorDB] Database chunk count for "${diaryName}": ${dbChunkCount}`);
        
        // âœ… æƒ…å†µ1ï¼šæ•°æ®åº“ä¸ºç©º â†’ Full Rebuildï¼ˆç¬¬ä¸€æ¬¡æ„å»ºï¼‰
        if (dbChunkCount === 0) {
            console.log(`[VectorDB] Database empty for "${diaryName}" â†’ Full Rebuild (first build)`);
            return true;
        }
        
        // âœ… æ­¥éª¤2ï¼šæ•°æ®åº“æœ‰æ•°æ®ï¼Œæ£€æŸ¥ç´¢å¼•æ–‡ä»¶
        if (!indexExists) {
            console.log(`[VectorDB] Index file missing for "${diaryName}" but database has ${dbChunkCount} chunks`);
            console.log(`[VectorDB] â†’ Atomic repair: rebuilding index from database`);
            await this.rebuildIndexFromDatabase(diaryName);
            return false; // ä¿®å¤å®Œæˆï¼Œæ— éœ€update
        }

        // âœ… æ­¥éª¤3ï¼šæ£€æŸ¥æ•°æ®ä¸€è‡´æ€§ï¼ˆç´¢å¼• vs æ•°æ®åº“ï¼‰
        // âš ï¸ å…³é”®ä¼˜åŒ–ï¼šæ ¹æ®å·®å¼‚å¤§å°å†³å®šä¿®å¤ç­–ç•¥
        console.log(`[VectorDB] Checking data consistency for "${diaryName}"...`);
        try {
            // âœ… ä½¿ç”¨åˆå§‹åŒ–æ—¶ç¼“å­˜çš„ç»´åº¦ï¼ˆé‡‘æ ‡å‡†ï¼‰
            const tempIndex = new HierarchicalNSW('l2', this.embeddingDimensions);
            console.log(`[VectorDB] Reading index file for "${diaryName}"...`);
            tempIndex.readIndexSync(indexPath);
            const indexElementCount = tempIndex.getCurrentCount();
            console.log(`[VectorDB] Index element count for "${diaryName}": ${indexElementCount}`);
            
            if (Math.abs(indexElementCount - dbChunkCount) > 0) {
                const diff = Math.abs(indexElementCount - dbChunkCount);
                const diffRatio = dbChunkCount > 0 ? diff / dbChunkCount : 1.0;
                
                console.warn(`[VectorDB] Data inconsistency for "${diaryName}": index=${indexElementCount}, db=${dbChunkCount} (diff=${diff}, ${(diffRatio*100).toFixed(1)}%)`);
                
                // âœ… æ ¹æ®å·®å¼‚å¤§å°å†³å®šç­–ç•¥
                if (diffRatio > 0.1) {
                    // å·®å¼‚>10%ï¼šåˆ é™¤ç´¢å¼•ï¼Œè§¦å‘å®Œæ•´é‡å»º
                    console.log(`[VectorDB] â†’ Large inconsistency (${(diffRatio*100).toFixed(1)}%), deleting index to trigger rebuild`);
                    try {
                        await fs.unlink(indexPath);
                        console.log(`[VectorDB] âœ… Stale index deleted, will trigger full rebuild`);
                    } catch (unlinkError) {
                        console.error(`[VectorDB] Failed to delete index:`, unlinkError.message);
                    }
                    return true; // è§¦å‘é‡å»º
                } else {
                    // å·®å¼‚â‰¤10%ï¼šé€šè¿‡diffä¿®å¤ï¼ˆä¿¡ä»»æ•°æ®åº“ï¼ŒåŒæ­¥ç´¢å¼•ï¼‰
                    console.log(`[VectorDB] â†’ Minor inconsistency (${diff} chunks), will sync index with database through diff`);
                    // âœ… å…³é”®ä¿®å¤ï¼šå¯¹å¤§å‹æ•°æ®é›†ï¼Œå¼‚æ­¥ä¿®å¤ï¼Œä¸é˜»å¡åˆå§‹åŒ–
                    if (dbChunkCount > 100000) {
                        console.log(`[VectorDB] âš ï¸ Large dataset detected (${dbChunkCount} chunks), skipping sync during initialization`);
                        console.log(`[VectorDB] Index will be lazily loaded and synced on first use`);
                        return false; // å»¶è¿Ÿä¿®å¤ï¼Œä¸é˜»å¡åˆå§‹åŒ–
                    }
                    
                    // âœ… å°å‹æ•°æ®é›†ï¼šåŒæ­¥ä¿®å¤
                    try {
                        console.log(`[VectorDB] Starting sync for "${diaryName}"...`);
                        await this.syncIndexWithDatabase(diaryName);
                        console.log(`[VectorDB] âœ… Index synced successfully for "${diaryName}"`);
                        return false; // ä¿®å¤å®Œæˆï¼Œæ— éœ€è§¦å‘æ›´æ–°
                    } catch (err) {
                        console.error(`[VectorDB] Failed to sync index for "${diaryName}":`, err.message);
                        console.log(`[VectorDB] â†’ Will trigger full rebuild as fallback`);
                        return true; // ä¿®å¤å¤±è´¥ï¼Œè§¦å‘å®Œæ•´é‡å»º
                    }
                }
            }
        } catch (error) {
            console.error(`[VectorDB] Failed to read index for "${diaryName}":`, error.message);
            console.log(`[VectorDB] â†’ Deleting corrupted index file`);
            
            // âœ… ç´¢å¼•æŸåï¼šåˆ é™¤å¹¶è§¦å‘é‡å»º
            try {
                await fs.unlink(indexPath);
                console.log(`[VectorDB] âœ… Corrupted index deleted, will trigger rebuild`);
            } catch (unlinkError) {
                if (unlinkError.code !== 'ENOENT') {
                    console.error(`[VectorDB] Failed to delete index:`, unlinkError.message);
                }
            }
            
            return true; // è§¦å‘é‡å»º
        }

        // âœ… æ­¥éª¤4ï¼šæ£€æŸ¥æ–‡ä»¶å˜åŒ–ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°ï¼‰
        const diaryManifest = this.storage.getFileHashes(diaryName);
        const files = await fs.readdir(diaryPath);
        const relevantFiles = files.filter(f => f.toLowerCase().endsWith('.txt') || f.toLowerCase().endsWith('.md'));

        if (Object.keys(diaryManifest).length !== relevantFiles.length) {
            console.log(`[VectorDB] File count changed for "${diaryName}": ${Object.keys(diaryManifest).length} â†’ ${relevantFiles.length}`);
            return true; // æœ‰å˜åŒ–ï¼Œè¿›å…¥scheduleDiaryBookProcessingåˆ¤æ–­å¢é‡/å…¨é‡
        }

        for (const file of relevantFiles) {
            const oldFileHash = diaryManifest[file];
            if (!oldFileHash) {
                console.log(`[VectorDB] New file detected: "${file}" in "${diaryName}"`);
                return true;
            }

            const filePath = path.join(diaryPath, file);
            const content = await fs.readFile(filePath, 'utf-8');
            const currentFileHash = crypto.createHash('md5').update(content).digest('hex');
            
            if (oldFileHash !== currentFileHash) {
                console.log(`[VectorDB] File changed: "${file}" in "${diaryName}"`);
                return true;
            }
        }
        
        this.debugLog(`[VectorDB] "${diaryName}" is up-to-date`);
        return false;
    }

    async calculateChanges(diaryName) {
        const diaryPath = path.join(DIARY_ROOT_PATH, diaryName);
        
        // âœ… å†æ¬¡æ£€æŸ¥ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰- å¤„ç†æ‰€æœ‰è®¿é—®é”™è¯¯
        try {
            await fs.access(diaryPath);
        } catch (error) {
            // âœ… ä¿®å¤ï¼šå¤„ç†æ‰€æœ‰å¯èƒ½çš„ç›®å½•ä¸å­˜åœ¨/æ— æƒé™é”™è¯¯
            if (error.code === 'ENOENT' || error.code === 'EPERM' || error.code === 'EACCES') {
                console.log(`[VectorDB][calculateChanges] Directory "${diaryName}" is not accessible (${error.code}). Treating as deleted.`);
                // âœ… è¿”å›ç‰¹æ®Šæ ‡è®°ï¼Œè¡¨ç¤ºç›®å½•å·²åˆ é™¤
                return {
                    diaryName,
                    chunksToAdd: [],
                    labelsToDelete: [],
                    newFileHashes: {},
                    directoryDeleted: true  // âœ… æ·»åŠ æ ‡è®°
                };
            }
            throw error;
        }
        
        const newFileHashes = {};
        const oldChunkMap = this.storage.getChunkMap(diaryName);
        
        // âœ… æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ï¼šæ”¶é›†æŸåæ¡ç›®å¯¹åº”çš„æºæ–‡ä»¶
        // è®©è¿™äº›æŸåæ–‡ä»¶è¿›å…¥æ­£å¸¸çš„diffæµç¨‹ï¼Œç”±å˜åŒ–ç‡åˆ¤æ–­æ˜¯å¦è§¦å‘é‡å»º
        const corruptedSourceFiles = new Set();
        let corruptedCount = 0;
        
        for (const [label, data] of Object.entries(oldChunkMap)) {
            // æ£€æŸ¥å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
            if (!data || !data.text || !data.sourceFile || !data.chunkHash) {
                corruptedCount++;
                if (data && data.sourceFile) {
                    corruptedSourceFiles.add(data.sourceFile);
                }
                this.debugLog(`Corrupted entry at label ${label}:`, {
                    hasText: !!data?.text,
                    hasSourceFile: !!data?.sourceFile,
                    hasChunkHash: !!data?.chunkHash
                });
            }
        }
        
        if (corruptedCount > 0) {
            const totalEntries = Object.keys(oldChunkMap).length;
            console.warn(`[VectorDB] Found ${corruptedCount}/${totalEntries} corrupted entries in "${diaryName}"`);
            console.warn(`[VectorDB] Will rebuild ${corruptedSourceFiles.size} affected files through diff: ${Array.from(corruptedSourceFiles).slice(0, 5).join(', ')}${corruptedSourceFiles.size > 5 ? '...' : ''}`);
        }
        // âœ… ä¿®å¤ï¼šåŸºäº(sourceFile + chunkIndex)æ¥åˆ¤æ–­å˜åŒ–ï¼Œè€Œéhashå»é‡
        // æ„å»ºå½“å‰æ–‡ä»¶çš„chunkåˆ—è¡¨ï¼šfile -> chunks[]
        const currentFileChunks = new Map(); // file -> [{text, chunkHash, index}]
        const files = await fs.readdir(diaryPath);
        const relevantFiles = files.filter(f => f.toLowerCase().endsWith('.txt') || f.toLowerCase().endsWith('.md'));

        for (const file of relevantFiles) {
            const filePath = path.join(diaryPath, file);
            const content = await fs.readFile(filePath, 'utf-8');
            newFileHashes[file] = crypto.createHash('md5').update(content).digest('hex');
            const chunks = chunkText(content);
            
            const fileChunkList = [];
            for (let i = 0; i < chunks.length; i++) {
                const chunk = chunks[i];
                const chunkHash = crypto.createHash('sha256').update(chunk).digest('hex');
                fileChunkList.push({
                    text: chunk,
                    chunkHash: chunkHash,
                    index: i
                });
            }
            currentFileChunks.set(file, fileChunkList);
        }

        // âœ… æ„å»ºæ—§æ•°æ®çš„æ–‡ä»¶->chunksæ˜ å°„
        const oldFileChunks = new Map(); // file -> Map(label -> chunkData)
        for (const [label, data] of Object.entries(oldChunkMap)) {
            if (!oldFileChunks.has(data.sourceFile)) {
                oldFileChunks.set(data.sourceFile, new Map());
            }
            oldFileChunks.get(data.sourceFile).set(Number(label), data);
        }

        const chunksToAdd = [];
        const labelsToDelete = [];

        // âœ… æ£€æµ‹éœ€è¦åˆ é™¤çš„ï¼šæ—§æ–‡ä»¶ä¸å†å­˜åœ¨ï¼Œæ–‡ä»¶å†…å®¹å˜åŒ–ï¼Œæˆ–æ–‡ä»¶æ•°æ®æŸå
        for (const [file, oldLabels] of oldFileChunks.entries()) {
            const currentChunks = currentFileChunks.get(file);
            
            if (!currentChunks) {
                // æ–‡ä»¶å·²åˆ é™¤ï¼Œåˆ é™¤æ‰€æœ‰ç›¸å…³chunks
                for (const label of oldLabels.keys()) {
                    labelsToDelete.push(label);
                }
            } else {
                // æ–‡ä»¶å­˜åœ¨ï¼Œæ£€æŸ¥æ–‡ä»¶hashæ˜¯å¦å˜åŒ–ï¼Œæˆ–è€…è¯¥æ–‡ä»¶æœ‰æŸåçš„æ•°æ®
                const oldFileHash = this.storage.getFileHashes(diaryName)[file];
                const newFileHash = newFileHashes[file];
                const isCorrupted = corruptedSourceFiles.has(file);
                
                if (oldFileHash !== newFileHash || isCorrupted) {
                    // æ–‡ä»¶å†…å®¹å˜åŒ–æˆ–æ•°æ®æŸåï¼Œåˆ é™¤æ‰€æœ‰æ—§chunksï¼ˆåé¢ä¼šé‡æ–°æ·»åŠ ï¼‰
                    if (isCorrupted) {
                        console.log(`[VectorDB] Rebuilding corrupted file "${file}" in "${diaryName}"`);
                    }
                    for (const label of oldLabels.keys()) {
                        labelsToDelete.push(label);
                    }
                }
            }
        }

        // âœ… æ£€æµ‹éœ€è¦æ·»åŠ çš„ï¼šæ–°æ–‡ä»¶ï¼Œæ–‡ä»¶å†…å®¹å˜åŒ–ï¼Œæˆ–æ–‡ä»¶æ•°æ®æŸåéœ€ä¿®å¤
        for (const [file, currentChunks] of currentFileChunks.entries()) {
            const oldFileHash = this.storage.getFileHashes(diaryName)[file];
            const newFileHash = newFileHashes[file];
            const isCorrupted = corruptedSourceFiles.has(file);
            
            if (!oldFileHash || oldFileHash !== newFileHash || isCorrupted) {
                // æ–°æ–‡ä»¶ã€æ–‡ä»¶å†…å®¹å˜åŒ–ï¼Œæˆ–æ•°æ®æŸåéœ€ä¿®å¤ï¼Œæ·»åŠ æ‰€æœ‰chunks
                for (const chunkData of currentChunks) {
                    chunksToAdd.push({
                        text: chunkData.text,
                        sourceFile: file,
                        chunkHash: chunkData.chunkHash
                    });
                }
            }
        }

        return { diaryName, chunksToAdd, labelsToDelete, newFileHashes };
    }

    async getEmbeddings(chunks) {
        return getEmbeddingsInWorker(chunks, {
            apiKey: this.apiKey,
            apiUrl: this.apiUrl,
            embeddingModel: this.embeddingModel,
        });
    }

    async getEmbeddingsWithRetry(chunks) {
        let lastError;
        for (let attempt = 1; attempt <= this.config.retryAttempts; attempt++) {
            try {
                return await this.getEmbeddings(chunks);
            } catch (error) {
                lastError = error;
                console.log(`[VectorDB] Embedding attempt ${attempt} failed:`, error.message);
                if (attempt < this.config.retryAttempts) {
                    const delay = Math.min(this.config.retryBaseDelay * Math.pow(2, attempt - 1), this.config.retryMaxDelay);
                    console.log(`[VectorDB] Retrying in ${delay}ms...`);
                    await new Promise(resolve => setTimeout(resolve, delay));
                }
            }
        }
        throw new Error(`Failed to get embeddings after ${this.config.retryAttempts} attempts: ${lastError.message}`);
    }

    async scheduleDiaryBookProcessing(diaryName) {
        // âœ… ä¿®å¤ï¼šç»Ÿä¸€åœ¨å…¥å£å¤„ç®¡ç† activeWorkers
        // å¿«é€Ÿæ£€æŸ¥ï¼ˆæ— é”ï¼Œå¿«é€Ÿæ‹’ç»ï¼‰
        if (this.activeWorkers.has(diaryName)) {
            console.log(`[VectorDB] Processing for "${diaryName}" is already in progress. Skipping.`);
            return;
        }

        // âœ… ä½¿ç”¨ç‹¬ç«‹çš„è°ƒåº¦é”ï¼Œé˜²æ­¢å¹¶å‘è°ƒåº¦
        const lockKey = `schedule_${diaryName}`;
        await this.acquireLock(lockKey);
        try {
            // åŒé‡æ£€æŸ¥ï¼ˆæŒæœ‰é”åå†æ£€æŸ¥ä¸€æ¬¡ï¼‰
            if (this.activeWorkers.has(diaryName)) {
                return;
            }
            
            // âœ… ç«‹å³æ ‡è®°ä¸ºæ´»åŠ¨çŠ¶æ€ï¼Œé˜²æ­¢é‡å¤è°ƒåº¦
            this.activeWorkers.add(diaryName);
            console.log(`[VectorDB] Marked "${diaryName}" as active worker`);
        } finally {
            this.releaseLock(lockKey);
        }

        // âœ… Bugä¿®å¤4ï¼šè¿½è¸ªWorkeræ˜¯å¦æ¥ç®¡ç®¡ç†
        let workerTookOver = false;
        
        // âœ… æ— è®ºåç»­æ‰§è¡Œä»€ä¹ˆè·¯å¾„ï¼Œéƒ½è¦ç¡®ä¿æ¸…ç† activeWorkers
        try {
            const diaryPath = path.join(DIARY_ROOT_PATH, diaryName);
            
            try {
                await fs.access(diaryPath);
            } catch (error) {
                if (error.code === 'ENOENT') {
                    console.log(`[VectorDB] Directory "${diaryName}" no longer exists. Cleaning up resources...`);
                    await this.cleanupDeletedDiary(diaryName);
                    return;
                }
                throw error;
            }

            // âœ… ä»æ•°æ®åº“è¯»å–æ—§chunkæ•°é‡
            const oldChunkMap = this.storage.getChunkMap(diaryName);
            let totalOldChunks = Object.keys(oldChunkMap).length;

            console.log(`[VectorDB] Calculating changes for "${diaryName}"...`);
            const changeset = await this.calculateChanges(diaryName);
            const { chunksToAdd, labelsToDelete, forceFullRebuild, directoryDeleted } = changeset;

            // âœ… æ–°å¢ï¼šæ£€æŸ¥ç›®å½•æ˜¯å¦å·²è¢«åˆ é™¤
            if (directoryDeleted) {
                console.log(`[VectorDB] Directory "${diaryName}" was deleted during processing. Cleaning up...`);
                await this.cleanupDeletedDiary(diaryName);
                return;
            }

            if (forceFullRebuild) {
                console.log(`[VectorDB] Full rebuild forced for "${diaryName}" due to data integrity issues.`);
                // âœ… Bugä¿®å¤4ï¼šåˆ é™¤åç«‹å³è½¬äº¤
                this.activeWorkers.delete(diaryName);
                const workerStarted = this.runFullRebuildWorker(diaryName);
                if (workerStarted) {
                    workerTookOver = true; // âœ… Workeræ¥ç®¡ç®¡ç†
                }
                return;
            }
            
            const changeRatio = totalOldChunks > 0 ? (chunksToAdd.length + labelsToDelete.length) / totalOldChunks : 1.0;
     
            if (totalOldChunks === 0 || changeRatio > this.config.changeThreshold) {
                console.log(`[VectorDB] Major changes detected (${(changeRatio * 100).toFixed(1)}%). Scheduling a full rebuild for "${diaryName}".`);
                // âœ… Bugä¿®å¤4ï¼šåˆ é™¤åç«‹å³è½¬äº¤
                this.activeWorkers.delete(diaryName);
                const workerStarted = this.runFullRebuildWorker(diaryName);
                if (workerStarted) {
                    workerTookOver = true; // âœ… Workeræ¥ç®¡ç®¡ç†
                }
                return;
            } else if (chunksToAdd.length > 0 || labelsToDelete.length > 0) {
                console.log(`[VectorDB] Minor changes detected. Applying incremental update for "${diaryName}".`);
                // âœ… activeWorkerså·²ç»åœ¨å…¥å£å¤„æ·»åŠ äº†ï¼Œç›´æ¥æ‰§è¡Œ
                await this.applyChangeset(changeset);
                // âœ… æˆåŠŸååœ¨finallyå—ç»Ÿä¸€æ¸…ç†
            } else {
                console.log(`[VectorDB] No effective changes detected for "${diaryName}". Nothing to do.`);
                // âœ… æ— æ“ä½œï¼Œåœ¨finallyå—ç»Ÿä¸€æ¸…ç†
            }
        } catch (error) {
            console.error(`[VectorDB] Failed to process diary book "${diaryName}":`, error);
            // âœ… é”™è¯¯ä¼šåœ¨finallyå—ç»Ÿä¸€æ¸…ç†
        } finally {
            // âœ… Bugä¿®å¤4ï¼šåªæœ‰éWorkeråˆ†æ”¯æ‰æ¸…ç†
            if (!workerTookOver && this.activeWorkers.has(diaryName)) {
                this.activeWorkers.delete(diaryName);
                console.log(`[VectorDB] Cleared activeWorker for "${diaryName}"`);
            }
        }
    }

    runFullRebuildWorker(diaryName) {
        // âœ… é˜²æ­¢é‡å¤å¯åŠ¨ Workerï¼ˆåŒé‡æ£€æŸ¥ï¼‰
        // æ³¨æ„ï¼šè°ƒç”¨è€…åº”è¯¥å·²ç»æ£€æŸ¥è¿‡activeWorkersï¼Œä½†è¿™é‡Œå†æ£€æŸ¥ä¸€æ¬¡ä»¥é˜²ä¸‡ä¸€
        if (this.activeWorkers.has(diaryName)) {
            console.log(`[VectorDB] Full rebuild worker for "${diaryName}" is already active. Skipping duplicate request.`);
            return false; // âœ… è¿”å›falseè¡¨ç¤ºæœªå¯åŠ¨
        }
        
        console.log(`[VectorDB] Preparing to start full rebuild worker for "${diaryName}"`);
        
        // âœ… éªŒè¯é…ç½®
        if (!this.apiKey || !this.apiUrl || !this.embeddingModel) {
            console.error(`[VectorDB] âŒ Missing required config for worker:`);
            console.error(`  API Key: ${this.apiKey ? 'Present' : 'MISSING'}`);
            console.error(`  API URL: ${this.apiUrl || 'MISSING'}`);
            console.error(`  Embedding Model: ${this.embeddingModel || 'MISSING'}`);
            this.storage.recordFailedRebuild(diaryName, 'Missing API configuration');
            return false; // âœ… è¿”å›falseè¡¨ç¤ºæœªå¯åŠ¨
        }
        
        const workerConfig = {
            apiKey: this.apiKey,
            apiUrl: this.apiUrl,
            embeddingModel: this.embeddingModel,
            expectedDimensions: this.expectedDimensions, // âœ… ä¼ é€’æœŸæœ›ç»´åº¦ç»™Worker
            retryAttempts: this.config.retryAttempts,
            retryBaseDelay: this.config.retryBaseDelay,
            retryMaxDelay: this.config.retryMaxDelay,
        };
        
        console.log(`[VectorDB] Worker config:`, {
            apiUrl: workerConfig.apiUrl,
            embeddingModel: workerConfig.embeddingModel,
            apiKeyPresent: !!workerConfig.apiKey,
            retryAttempts: workerConfig.retryAttempts,
        });
        
        // âœ… åˆ›å»ºWorkerï¼ˆåˆ›å»ºæˆåŠŸåç«‹å³æ ‡è®°ä¸ºæ´»åŠ¨ï¼‰
        let worker;
        try {
            worker = new Worker(path.resolve(__dirname, 'vectorizationWorker.js'), {
                workerData: {
                    task: 'fullRebuild',
                    diaryName,
                    config: workerConfig
                }
            });
            
            // âœ… Workeråˆ›å»ºæˆåŠŸï¼Œç«‹å³æ ‡è®°ä¸ºæ´»åŠ¨ï¼ˆé˜²æ­¢é‡å¤åˆ›å»ºï¼‰
            // è¿™ä¸ªæ ‡è®°ä¼šåœ¨Workerçš„exitäº‹ä»¶ä¸­æ¸…ç†
            this.activeWorkers.add(diaryName);
            console.log(`[VectorDB] âœ… Worker thread created and marked active for "${diaryName}"`);
        } catch (createError) {
            console.error(`[VectorDB] âŒ Failed to create worker for "${diaryName}":`, createError);
            this.storage.recordFailedRebuild(diaryName, `Worker creation failed: ${createError.message}`);
            // âœ… åˆ›å»ºå¤±è´¥ï¼Œä¸éœ€è¦æ¸…ç†activeWorkersï¼ˆå› ä¸ºä»æœªæ·»åŠ ï¼‰
            return false;
        }

        worker.on('message', (message) => {
            console.log(`[VectorDB] Received message from worker for "${diaryName}":`, {
                status: message.status,
                task: message.task,
                error: message.error || 'none'
            });
            
            if (message.status === 'success' && message.task === 'fullRebuild') {
                this.storage.updateFileHashes(message.diaryName, message.newManifestEntry);
                // âœ… æ¸…é™¤å¤±è´¥é‡å»ºè®°å½•
                this.storage.clearFailedRebuild(message.diaryName);
                
                // âœ… å…³é”®ä¿®å¤ï¼šæ¸…é™¤å†…å­˜ä¸­çš„ç´¢å¼•ï¼Œå¼ºåˆ¶ä¸‹æ¬¡æœç´¢æ—¶é‡æ–°åŠ è½½
                this.indices.delete(message.diaryName);
                this.chunkMaps.delete(message.diaryName);
                console.log(`[VectorDB] Cleared in-memory cache for "${message.diaryName}" to force reload`);
                
                this.stats.lastUpdateTime = new Date().toISOString();
                console.log(`[VectorDB] âœ… Worker successfully completed full rebuild for "${message.diaryName}".`);
            } else if (message.status === 'error') {
                // âœ… æ£€æŸ¥æ˜¯å¦æ˜¯é™æµå¯¼è‡´çš„æš‚åœ
                if (message.isPauseError) {
                    const pauseMinutes = Math.round(this.config.rateLimitPauseMs / 60000);
                    console.warn(`[VectorDB] â¸ï¸ Worker paused due to rate limit for "${message.diaryName}"`);
                    console.warn(`[VectorDB] Progress: ${message.processedFiles}/${message.totalFiles} files completed`);
                    console.warn(`[VectorDB] ğŸ˜´ Taking a ${pauseMinutes}-minute break to let API quota recover...`);
                    
                    // âœ… æ ¸å¿ƒæ”¹è¿›ï¼šä¼‘æ¯åè‡ªåŠ¨é‡å¯Workerç»§ç»­å·¥ä½œ
                    setTimeout(() => {
                        console.log(`[VectorDB] â° Break time over! Resuming work on "${message.diaryName}"...`);
                        this.runFullRebuildWorker(message.diaryName);
                    }, this.config.rateLimitPauseMs);
                } else {
                    console.error(`[VectorDB] âŒ Worker failed for "${message.diaryName}":`, message.error);
                    console.error(`[VectorDB] Error stack:`, message.stack);
                    this.storage.recordFailedRebuild(message.diaryName, message.error);
                }
            } else {
                console.error(`[VectorDB] âŒ Worker returned unknown status for "${message.diaryName}":`, message);
                this.storage.recordFailedRebuild(diaryName, 'Unknown worker failure');
            }
        });

        worker.on('error', (error) => {
            // âœ… æ£€æŸ¥æ˜¯å¦æ˜¯é™æµå¯¼è‡´çš„æš‚åœ
            if (error.isPauseError) {
                console.warn(`[VectorDB] â¸ï¸ Worker paused due to rate limit for "${diaryName}"`);
                console.warn(`[VectorDB] This is not an error - progress has been saved`);
            } else {
                console.error(`[VectorDB] âŒ Worker error for "${diaryName}":`, error);
                console.error(`[VectorDB] Error stack:`, error.stack);
                console.error(`[VectorDB] Error name:`, error.name);
                console.error(`[VectorDB] Error code:`, error.code);
                this.storage.recordFailedRebuild(diaryName, error.message);
            }
            // âœ… Worker åœæ­¢æ—¶æ¸…ç†
            this.activeWorkers.delete(diaryName);
        });
        
        worker.on('exit', (code) => {
            // âœ… ç¡®ä¿æ¸…ç†ï¼ˆå¹‚ç­‰æ“ä½œï¼‰
            this.activeWorkers.delete(diaryName);
            const exitTime = new Date().toISOString();
            console.log(`[VectorDB] Worker exited for "${diaryName}" at ${exitTime} with code: ${code}`);
            if (code !== 0) {
                // âœ… é€€å‡ºç 1é€šå¸¸è¡¨ç¤ºé™æµæš‚åœï¼ˆæ­£å¸¸æƒ…å†µï¼‰
                // åªæœ‰å…¶ä»–é€€å‡ºç æ‰è®°å½•ä¸ºå¤±è´¥
                if (code === 1) {
                    console.log(`[VectorDB] â¸ï¸ Worker paused for "${diaryName}" (likely due to rate limit)`);
                } else {
                    console.error(`[VectorDB] âŒ Worker exited with non-zero code ${code} for "${diaryName}"`);
                    this.storage.recordFailedRebuild(diaryName, `Worker exit code ${code}`);
                }
            } else {
                console.log(`[VectorDB] âœ… Worker exited normally for "${diaryName}"`);
            }
        });
        
        // âœ… è¿”å›trueè¡¨ç¤ºWorkerå·²å¯åŠ¨
        return true;
    }

    /**
     * âœ… æ–°å¢æ–¹æ³•ï¼šæ¸…ç†å·²åˆ é™¤æ—¥è®°æœ¬çš„æ‰€æœ‰èµ„æº
     */
    async cleanupDeletedDiary(diaryName) {
        await this.acquireLock(diaryName); // âœ… åŠ é”
        try {
            const safeFileNameBase = Buffer.from(diaryName, 'utf-8').toString('base64url');
            const indexPath = path.join(VECTOR_STORE_PATH, `${safeFileNameBase}.bin`);
            // âš ï¸ ä¿ç•™å¯¹æ—§JSONæ–‡ä»¶çš„æ¸…ç†ï¼ˆå‘åå…¼å®¹ï¼‰
            const mapPath = path.join(VECTOR_STORE_PATH, `${safeFileNameBase}_map.json`);

            // 1. ä»å†…å­˜ä¸­åˆ é™¤ç´¢å¼•å’Œå—æ˜ å°„
            this.indices.delete(diaryName);
            this.chunkMaps.delete(diaryName);
            this.lruCache.delete(diaryName);
            this.activeWorkers.delete(diaryName); // âœ… æ·»åŠ è¿™è¡Œï¼
            console.log(`[VectorDB] Removed "${diaryName}" from in-memory indices.`);

            // 2. åˆ é™¤å‘é‡å­˜å‚¨æ–‡ä»¶
            const deletePromises = [];
            
            if (await this.fileExists(indexPath)) {
                deletePromises.push(
                    fs.unlink(indexPath)
                        .then(() => console.log(`[VectorDB] Deleted index file for "${diaryName}".`))
                        .catch(e => console.warn(`[VectorDB] Failed to delete index file:`, e.message))
                );
            }

            if (await this.fileExists(mapPath)) {
                deletePromises.push(
                    fs.unlink(mapPath)
                        .then(() => console.log(`[VectorDB] Deleted map file for "${diaryName}".`))
                        .catch(e => console.warn(`[VectorDB] Failed to delete map file:`, e.message))
                );
            }

            await Promise.all(deletePromises);

            // 3. ä»æ•°æ®åº“ä¸­åˆ é™¤æ‰€æœ‰ç›¸å…³æ•°æ®
            this.storage.deleteDiary(diaryName);
            console.log(`[VectorDB] Removed "${diaryName}" from database.`);

            console.log(`[VectorDB] Successfully cleaned up all resources for deleted diary "${diaryName}".`);
        } catch (error) {
            console.error(`[VectorDB] Error during cleanup of "${diaryName}":`, error);
        } finally {
            this.releaseLock(diaryName); // âœ… é‡Šæ”¾é”
        }
    }

    watchDiaries() {
        console.log(`[VectorDB] Setting up file watcher for: ${DIARY_ROOT_PATH}`);
        
        const watcher = chokidar.watch(DIARY_ROOT_PATH, {
            ignored: /(^|[\/\\])\../,
            persistent: true,
            ignoreInitial: true,
            depth: 1,
        });

        const pendingChanges = new Map(); // diaryName â†’ timeoutId
        
        // âœ… æ·»åŠ ç›‘æ§å™¨çŠ¶æ€æ—¥å¿—
        watcher.on('ready', () => {
            console.log(`[VectorDB] File watcher is ready and monitoring for changes`);
            const watched = watcher.getWatched();
            console.log(`[VectorDB] Watching ${Object.keys(watched).length} directories`);
            this.debugLog(`Watched paths:`, watched);
        });
        
        watcher.on('error', (error) => {
            console.error(`[VectorDB] File watcher error:`, error);
        });

        const handleFileChange = (filePath) => {
            // âœ… ä¼˜åŒ–ï¼šå…ˆæå– diary nameï¼Œå†æ£€æŸ¥æ˜¯å¦åº”è¯¥å¿½ç•¥ï¼ˆé¿å…æ‰“å°ä¸å¿…è¦çš„æ—¥å¿—ï¼‰
            const diaryName = path.basename(path.dirname(filePath));
            
            // âœ… æå‰è¿‡æ»¤ï¼šå¦‚æœæ˜¯æ’é™¤çš„æ–‡ä»¶å¤¹ï¼Œç›´æ¥è¿”å›ï¼Œä¸æ‰“å°ä»»ä½•æ—¥å¿—
            if (diaryName.startsWith('å·²æ•´ç†') || diaryName === 'VCPè®ºå›') {
                return;
            }
            
            // âœ… åªæœ‰éæ’é™¤çš„æ–‡ä»¶æ‰æ‰“å°æ—¥å¿—
            console.log(`[VectorDB] File change detected: ${filePath}`);
            console.log(`[VectorDB] Extracted diary name: "${diaryName}" from path: ${filePath}`);
            
            // âœ… å¦‚æœå·²ç»åœ¨å¤„ç†ä¸­ï¼Œå¿½ç•¥æ–‡ä»¶å˜æ›´
            if (this.activeWorkers.has(diaryName)) {
                console.log(`[VectorDB] File change ignored for "${diaryName}" - already processing`);
                return;
            }

            // âœ… å¦‚æœå·²ç»æœ‰å¾…å¤„ç†çš„å®šæ—¶å™¨ï¼Œå¿½ç•¥æ–°çš„å˜æ›´ï¼ˆé™¤éå·²ç»è¶…è¿‡2ç§’ï¼‰
            const existing = pendingChanges.get(diaryName);
            if (existing) {
                const elapsed = Date.now() - existing.startTime;
                if (elapsed < 2000) {
                    // åœ¨2ç§’å†…çš„å¤šæ¬¡å˜æ›´ï¼Œæ¸…é™¤æ—§å®šæ—¶å™¨ï¼Œé‡æ–°å¼€å§‹è®¡æ—¶
                    clearTimeout(existing.timeoutId);
                    console.log(`[VectorDB] Cleared previous debounce timer for "${diaryName}" (elapsed: ${elapsed}ms)`);
                } else {
                    // è¶…è¿‡2ç§’äº†ï¼Œè®©åŸå®šæ—¶å™¨ç»§ç»­æ‰§è¡Œ
                    console.log(`[VectorDB] Keeping existing debounce timer for "${diaryName}" (elapsed: ${elapsed}ms)`);
                    return;
                }
            }

            // è®¾ç½®æ–°çš„å®šæ—¶å™¨ï¼ˆ500ms é˜²æŠ–ï¼‰
            const startTime = Date.now();
            console.log(`[VectorDB] Setting debounce timer for "${diaryName}" (500ms)`);
            const timeoutId = setTimeout(async () => {
                // âœ… ä¿®å¤ï¼šåœ¨æ‰§è¡Œå‰å†æ¬¡æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
                const diaryPath = path.join(DIARY_ROOT_PATH, diaryName);
                try {
                    await fs.access(diaryPath);
                    // ç›®å½•å­˜åœ¨ï¼Œæ­£å¸¸å¤„ç†
                    pendingChanges.delete(diaryName);
                    console.log(`[VectorDB] Debounce completed for "${diaryName}", starting processing`);
                    this.scheduleDiaryBookProcessing(diaryName);
                    this.cacheDiaryNameVectors();
                } catch (error) {
                    // âœ… ç›®å½•å·²è¢«åˆ é™¤ï¼Œå–æ¶ˆå¤„ç†å¹¶æ¸…ç†èµ„æº
                    if (error.code === 'ENOENT' || error.code === 'EPERM' || error.code === 'EACCES') {
                        pendingChanges.delete(diaryName);
                        console.log(`[VectorDB] Directory "${diaryName}" no longer exists after debounce. Cleaning up...`);
                        this.cleanupDeletedDiary(diaryName).catch(err => {
                            console.error(`[VectorDB] Error cleaning up "${diaryName}":`, err);
                        });
                        this.cacheDiaryNameVectors();
                    } else {
                        console.error(`[VectorDB] Unexpected error checking directory "${diaryName}":`, error);
                    }
                }
            }, 500);

            pendingChanges.set(diaryName, { timeoutId, startTime });
        };

        const handleDirUnlink = (dirPath) => {
            const diaryName = path.basename(dirPath);
            if (diaryName.startsWith('å·²æ•´ç†') || diaryName === 'VCPè®ºå›') {
                return;
            }

            // å–æ¶ˆå¾…å¤„ç†çš„æ›´æ–°
            if (pendingChanges.has(diaryName)) {
                clearTimeout(pendingChanges.get(diaryName));
                pendingChanges.delete(diaryName);
            }

            console.log(`[VectorDB] Directory deleted: ${diaryName}`);
            this.cleanupDeletedDiary(diaryName).catch(err => {
                console.error(`[VectorDB] Error cleaning up "${diaryName}":`, err);
            });
            this.cacheDiaryNameVectors();
        };

        watcher
            .on('add', (filePath) => {
                // âœ… ä¼˜åŒ–ï¼šä¸åœ¨è¿™é‡Œæ‰“å°æ—¥å¿—ï¼Œäº¤ç”± handleFileChange ç»Ÿä¸€å¤„ç†
                handleFileChange(filePath);
            })
            .on('change', (filePath) => {
                // âœ… ä¼˜åŒ–ï¼šä¸åœ¨è¿™é‡Œæ‰“å°æ—¥å¿—ï¼Œäº¤ç”± handleFileChange ç»Ÿä¸€å¤„ç†
                handleFileChange(filePath);
            })
            .on('unlink', (filePath) => {
                // âœ… ä¼˜åŒ–ï¼šä¸åœ¨è¿™é‡Œæ‰“å°æ—¥å¿—ï¼Œäº¤ç”± handleFileChange ç»Ÿä¸€å¤„ç†
                handleFileChange(filePath);
            })
            .on('unlinkDir', (dirPath) => {
                // âœ… ç›®å½•åˆ é™¤äº‹ä»¶ä»éœ€æ‰“å°æ—¥å¿—ï¼ˆé‡è¦æ“ä½œï¼‰
                console.log(`[VectorDB] Event: 'unlinkDir' - ${dirPath}`);
                handleDirUnlink(dirPath);
            });
        
        console.log(`[VectorDB] File watcher event handlers registered`);
    }

    /**
     * æ™ºèƒ½æ¸…ç†æ–‡æœ¬ä¸­çš„æ— æ„ä¹‰emojiå’Œç‰¹æ®Šå­—ç¬¦
     * ä¿ç•™æœ‰è¯­ä¹‰ä»·å€¼çš„ç¬¦å·
     */
    prepareTextForEmbedding(text) {
        // 1. ç§»é™¤çº¯è£…é¥°æ€§emoji
        const decorativeEmojis = /[\u{1F600}-\u{1F64F}\u{1F300}-\u{1F5FF}\u{1F680}-\u{1F6FF}\u{1F1E0}-\u{1F1FF}\u{2600}-\u{26FF}\u{2700}-\u{27BF}]/gu;
        
        // 2. ä¿ç•™æœ‰è¯­ä¹‰çš„æ ‡ç‚¹å’Œç¬¦å·ï¼ˆï¼ï¼Ÿã€‚ï¼Œç­‰ï¼‰
        let cleaned = text.replace(decorativeEmojis, ' ');
        
        // 3. æ¸…ç†å¤šä½™ç©ºæ ¼
        cleaned = cleaned.replace(/\s+/g, ' ').trim();
        
        // 4. å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œè¿”å›å ä½ç¬¦
        if (cleaned.length === 0) {
            return '[EMPTY_CONTENT]';
        }
        
        return cleaned;
    }

    /**
     * âœ… é€šç”¨åŸå­å†™å…¥æ–¹æ³•ï¼ˆWindows å…¼å®¹ + é˜²å¹¶å‘å†²çªï¼‰
     */
    async atomicWriteFile(filePath, data) {
        // âœ… ä½¿ç”¨æ—¶é—´æˆ³+éšæœºæ•°é¿å…å¹¶å‘å†²çª
        const timestamp = Date.now();
        const random = Math.random().toString(36).substring(2, 8);
        const tempPath = `${filePath}.tmp.${timestamp}.${random}`;
        
        try {
            // âœ… ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
            const dir = path.dirname(filePath);
            await fs.mkdir(dir, { recursive: true });
            
            // å†™å…¥ä¸´æ—¶æ–‡ä»¶
            await fs.writeFile(tempPath, data);
            
            // éªŒè¯å†™å…¥æˆåŠŸ
            if (!await this.fileExists(tempPath)) {
                throw new Error(`Failed to create temp file: ${tempPath}`);
            }
            
            const stats = await fs.stat(tempPath);
            if (stats.size === 0 && data.length > 0) {
                throw new Error(`Temp file is empty: ${tempPath}`);
            }
            
            // âœ… Windows å…¼å®¹ï¼šå…ˆåˆ é™¤å†é‡å‘½å
            try {
                await fs.unlink(filePath);
            } catch (e) {
                if (e.code !== 'ENOENT') {
                    throw e;
                }
            }
            
            // é‡å‘½åä¸´æ—¶æ–‡ä»¶
            await fs.rename(tempPath, filePath);
            
            // éªŒè¯æœ€ç»ˆæ–‡ä»¶å­˜åœ¨
            if (!await this.fileExists(filePath)) {
                throw new Error(`Final file was not created: ${filePath}`);
            }
            
            this.debugLog(`Atomically wrote to ${path.basename(filePath)}`);
            
        } catch (error) {
            console.error(`[VectorDB] Failed to write ${filePath}:`, {
                path: tempPath,
                dest: filePath,
                error: error.message,
                code: error.code
            });
            
            // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try {
                if (await this.fileExists(tempPath)) {
                    await fs.unlink(tempPath);
                }
            } catch (e) { /* ignore */ }
            
            throw error;
        }
    }

    async loadIndexForSearch(diaryName, dimensions) {
        // âœ… å…³é”®ä¿®å¤ï¼šåœ¨æ‰€æœ‰æ“ä½œå‰æ£€æŸ¥æ—¥è®°æœ¬æ˜¯å¦è¢«å¿½ç•¥
        if (diaryName.startsWith('å·²æ•´ç†') || diaryName === 'VCPè®ºå›') {
            this.debugLog(`[VectorDB] Attempted to load index for ignored diary "${diaryName}". Skipping.`);
            return false;
        }
        
        if (this.indices.has(diaryName)) {
            // âœ… ç›´æ¥ setï¼ŒMap çš„ set æ“ä½œæ˜¯åŸå­çš„
            this.lruCache.set(diaryName, { lastAccessed: Date.now() });
            return true;
        }

        await this.acquireLock(diaryName);
        try {
            // åŒé‡æ£€æŸ¥ï¼ˆdouble-check lockingï¼‰
            if (this.indices.has(diaryName)) {
                return true;
            }

            await this.manageMemory();

            const safeFileNameBase = Buffer.from(diaryName, 'utf-8').toString('base64url');
            const indexPath = path.join(VECTOR_STORE_PATH, `${safeFileNameBase}.bin`);

            // âœ… ä¿®å¤ï¼šåªæ£€æŸ¥ç´¢å¼•æ–‡ä»¶ï¼Œä¸æ£€æŸ¥JSONæ–‡ä»¶
            await fs.access(indexPath);

            if (!dimensions) {
                // âœ… ä½¿ç”¨ç¼“å­˜çš„ç»´åº¦ï¼ˆé‡‘æ ‡å‡†ï¼‰
                // å¦‚æœå†…å­˜ä¸­ä¸¢å¤±ï¼Œä»æ•°æ®åº“æ¢å¤
                if (!this.embeddingDimensions) {
                    this.embeddingDimensions = this.storage.getEmbeddingDimensions();
                    if (!this.embeddingDimensions) {
                        throw new Error('Embedding dimensions lost and no cache available');
                    }
                    console.warn(`[VectorDB] Recovered embedding dimensions from cache: ${this.embeddingDimensions}`);
                }
                dimensions = this.embeddingDimensions;
            }

            const index = new HierarchicalNSW('l2', dimensions);
            index.readIndexSync(indexPath);
            
            // âœ… ä»SQLiteæ•°æ®åº“è¯»å–chunkMap
            const chunkMap = this.storage.getChunkMap(diaryName);
            
            if (Object.keys(chunkMap).length === 0) {
                console.warn(`[VectorDB] ChunkMap is empty for "${diaryName}", index file exists but no data in database`);
                return false;
            }
            
            this.indices.set(diaryName, index);
            this.chunkMaps.set(diaryName, chunkMap);
            this.lruCache.set(diaryName, { lastAccessed: Date.now() });
            console.log(`[VectorDB] Lazily loaded index for "${diaryName}" into memory (${Object.keys(chunkMap).length} chunks).`);
            return true;
        } catch (error) {
            console.error(`[VectorDB] Failed to load index for "${diaryName}":`, error.message);
            return false;
        } finally {
            this.releaseLock(diaryName);
        }
    }

    async applyChangeset(changeset) {
        const { diaryName, chunksToAdd, labelsToDelete, newFileHashes } = changeset;

        await this.acquireLock(diaryName);
        
        // âœ… å®šä¹‰æ ‡å¿—ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘å…¨é‡é‡å»º
        let shouldTriggerFullRebuild = false;
        
        try {
            const safeFileNameBase = Buffer.from(diaryName, 'utf-8').toString('base64url');
            const indexPath = path.join(VECTOR_STORE_PATH, `${safeFileNameBase}.bin`);
            
            // âœ… å…³é”®ä¿®å¤ï¼šæ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            const indexExists = await this.fileExists(indexPath);
            
            // âœ… ä½¿ç”¨æ—¶é—´æˆ³å‘½åå¤‡ä»½æ–‡ä»¶ï¼Œé¿å…å†²çª
            const timestamp = Date.now();
            const backupIndexPath = `${indexPath}.backup.${timestamp}`;

            // ç¬¬ä¸€é˜¶æ®µï¼šåˆ›å»ºç´¢å¼•å¤‡ä»½ï¼ˆåªå¤‡ä»½ç´¢å¼•æ–‡ä»¶ï¼Œä¸å†å¤‡ä»½JSONï¼‰
            let hasBackup = false;
            try {
                if (indexExists) {
                    await fs.copyFile(indexPath, backupIndexPath);
                    hasBackup = true;
                    this.debugLog(`Created backup: ${path.basename(backupIndexPath)}`);
                }
            } catch (e) {
                this.debugLog(`Backup creation failed (probably first creation):`, e.message);
                hasBackup = false;
            }
            
            // âœ… ä»å†…å­˜æˆ–æ•°æ®åº“åŠ è½½ç´¢å¼•å’ŒchunkMap
            let index = this.indices.get(diaryName);
            let chunkMap = this.chunkMaps.get(diaryName);
            
            // å¦‚æœå†…å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ•°æ®åº“åŠ è½½
            if (!index || !chunkMap) {
                // âœ… ä»æ•°æ®åº“åŠ è½½chunkMapï¼ˆæ€»æ˜¯å¯ç”¨ï¼‰
                chunkMap = this.storage.getChunkMap(diaryName);
                
                if (indexExists) {
                    this.debugLog(`Found existing index file for "${diaryName}", will load if needed.`);
                } else {
                    this.debugLog(`No index file for "${diaryName}", will create new one.`);
                }
            }
            
            // âœ… ç»Ÿä¸€æ·±æ‹·è´ï¼ˆæ— è®ºæ¥æºæ˜¯å†…å­˜è¿˜æ˜¯æ–‡ä»¶ï¼‰
            chunkMap = JSON.parse(JSON.stringify(chunkMap || {}));

            const originalChunkMap = JSON.parse(JSON.stringify(chunkMap));

            // âœ… é¢„å¤„ç†ï¼šå…ˆè¿‡æ»¤å†å†³å®šæ“ä½œ
            let validChunksToAdd = [];
            let validTextsForEmbedding = [];
            
            if (chunksToAdd.length > 0) {
                const textsForEmbedding = chunksToAdd.map(c => this.prepareTextForEmbedding(c.text));
                validChunksToAdd = chunksToAdd.filter((_, i) => textsForEmbedding[i] !== '[EMPTY_CONTENT]');
                validTextsForEmbedding = validChunksToAdd.map(c => this.prepareTextForEmbedding(c.text));
                
                if (validChunksToAdd.length < chunksToAdd.length) {
                    console.warn(`[VectorDB] Filtered out ${chunksToAdd.length - validChunksToAdd.length} empty/emoji-only chunks for "${diaryName}"`);
                }
            }

            // âœ… ä¿®å¤ï¼šå¦‚æœç´¢å¼•æ–‡ä»¶ç¼ºå¤±ä½†æ•°æ®åº“æœ‰æ•°æ®ï¼Œå¼ºåˆ¶é‡å»ºç´¢å¼•
            const dbHasData = Object.keys(chunkMap).length > 0;
            
            if (validChunksToAdd.length === 0 && labelsToDelete.length === 0) {
                // æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®å¤ç¼ºå¤±çš„ç´¢å¼•æ–‡ä»¶
                if (!indexExists && dbHasData) {
                    console.log(`[VectorDB] Index file missing for "${diaryName}" but database has ${Object.keys(chunkMap).length} chunks. Rebuilding index...`);
                    // âœ… è§¦å‘åŸå­ä¿®å¤ï¼ˆä¸é˜»å¡ï¼Œæ ‡è®°ä¸ºéœ€è¦å¤„ç†ï¼‰
                    shouldTriggerFullRebuild = true;
                    return; // æå‰è¿”å›ï¼Œè®©finallyå—å¤„ç†
                } else {
                    console.log(`[VectorDB] No valid changes for "${diaryName}". Updating database only.`);
                    this.storage.updateFileHashes(diaryName, newFileHashes);
                    return;
                }
            }

            // åˆå§‹åŒ–ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if (!index) {
                const safeFileNameBase = Buffer.from(diaryName, 'utf-8').toString('base64url');
                const indexPath = path.join(VECTOR_STORE_PATH, `${safeFileNameBase}.bin`);
                
                // å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
                try {
                    await fs.access(indexPath);
                    
                    // è·å–dimensions
                    let dimensions;
                    if (validTextsForEmbedding.length > 0) {
                        const tempVector = await this.getEmbeddingsWithRetry([validTextsForEmbedding[0]]);
                        dimensions = tempVector[0].length;
                    } else {
                        const dummyEmbeddings = await this.getEmbeddingsWithRetry(["."]);
                        dimensions = dummyEmbeddings[0].length;
                    }
                    
                    index = new HierarchicalNSW('l2', dimensions);
                    index.readIndexSync(indexPath);
                    this.indices.set(diaryName, index);
                    this.lruCache.set(diaryName, { lastAccessed: Date.now() });
                    console.log(`[VectorDB] Loaded existing index for "${diaryName}" in applyChangeset.`);
                } catch (e) {
                    // âœ… ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½†æ•°æ®åº“æœ‰æ•°æ® â†’ æ•°æ®ä¸ä¸€è‡´ï¼Œéœ€è¦full rebuild
                    console.warn(`[VectorDB] Index file missing for "${diaryName}" but database has data - triggering full rebuild`);
                    shouldTriggerFullRebuild = true;
                    return; // âœ… æå‰è¿”å›ï¼Œè®©finallyå—è§¦å‘é‡å»º
                }
            }

            // ç¬¬äºŒé˜¶æ®µï¼šåˆ é™¤æ“ä½œ
            if (labelsToDelete.length > 0) {
                console.log(`[VectorDB] Deleting ${labelsToDelete.length} vectors from "${diaryName}".`);
                labelsToDelete.forEach(label => {
                    try {
                        index.markDelete(label);
                        delete chunkMap[label];
                    } catch (e) {
                        console.warn(`[VectorDB] Failed to delete label ${label}:`, e.message);
                    }
                });
            }

            // ç¬¬ä¸‰é˜¶æ®µï¼šè·å–embeddings
            let vectors = [];
            if (validTextsForEmbedding.length > 0) {
                try {
                    vectors = await this.getEmbeddingsWithRetry(validTextsForEmbedding);
                    
                    if (vectors.length !== validTextsForEmbedding.length) {
                        throw new Error(`Embedding count mismatch: expected ${validTextsForEmbedding.length}, got ${vectors.length}`);
                    }
                } catch (error) {
                    console.error(`[VectorDB] Embedding failed for "${diaryName}":`, error.message);
                    
                    // âœ… embeddingå¤±è´¥æ—¶ï¼Œæ•°æ®åº“å°šæœªä¿®æ”¹ï¼Œåªéœ€å›æ»šå†…å­˜
                    this.chunkMaps.set(diaryName, originalChunkMap);
                    
                    if (await this.fileExists(backupIndexPath)) {
                        await fs.copyFile(backupIndexPath, indexPath);
                        this.indices.delete(diaryName);
                    }
                    
                    throw error;
                }
            }

            // ç¬¬å››é˜¶æ®µï¼šæ·»åŠ æ–°å‘é‡
            if (vectors.length > 0) {
                console.log(`[VectorDB] Adding ${vectors.length} new vectors to "${diaryName}".`);
                let maxLabel = Object.keys(chunkMap).reduce((max, label) => Math.max(max, Number(label)), -1);
                
                // âœ… ä¿®å¤ï¼šæ‰‹åŠ¨è®¡ç®—å½“å‰æ•°é‡
                const currentCount = Object.keys(chunkMap).length;
                const requiredCapacity = currentCount + vectors.length;
                const currentCapacity = index.getMaxElements();

                if (requiredCapacity > currentCapacity) {
                    const newCapacity = Math.ceil(Math.max(requiredCapacity, currentCapacity * 1.2));
                    console.log(`[VectorDB] Resizing index from ${currentCapacity} to ${newCapacity}`);
                    index.resizeIndex(newCapacity);
                }

                for (let i = 0; i < vectors.length; i++) {
                    const newLabel = ++maxLabel;
                    const chunk = validChunksToAdd[i];
                    try {
                        index.addPoint(vectors[i], newLabel);
                        chunkMap[newLabel] = {
                            text: chunk.text,
                            sourceFile: chunk.sourceFile,
                            chunkHash: chunk.chunkHash
                        };
                    } catch (e) {
                        console.error(`[VectorDB] Failed to add point ${newLabel}:`, e.message);
                    }
                }
            }

            // ç¬¬äº”é˜¶æ®µï¼šä¿å­˜ç´¢å¼•æ–‡ä»¶å’Œæ•°æ®åº“ï¼ˆäº‹åŠ¡æ€§æ“ä½œï¼‰
            const timestamp2 = Date.now();
            const tempIndexPath = `${indexPath}.tmp.${timestamp2}`;
            
            let writeSuccess = false;
            
            try {
                // Step 1: å†™å…¥ä¸´æ—¶ç´¢å¼•æ–‡ä»¶
                this.debugLog(`Writing to temp index: ${path.basename(tempIndexPath)}`);
                await index.writeIndex(tempIndexPath);
                
                // Step 2: éªŒè¯ç´¢å¼•æ–‡ä»¶
                if (!await this.fileExists(tempIndexPath)) {
                    throw new Error(`Index file was not created at ${tempIndexPath}`);
                }
                const indexStats = await fs.stat(tempIndexPath);
                if (indexStats.size === 0) {
                    throw new Error(`Index file is empty: ${tempIndexPath}`);
                }
                
                // Step 3: ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆäº‹åŠ¡æ€§æ“ä½œï¼‰
                this.debugLog(`Saving chunkMap to database`);
                this.storage.saveChunks(diaryName, chunkMap);
                
                // Step 4: éªŒè¯æ•°æ®åº“ä¿å­˜
                const savedChunkMap = this.storage.getChunkMap(diaryName);
                if (Object.keys(savedChunkMap).length !== Object.keys(chunkMap).length) {
                    throw new Error(`Database save verification failed: expected ${Object.keys(chunkMap).length}, got ${Object.keys(savedChunkMap).length}`);
                }
                
                // Step 5: æ›¿æ¢ç´¢å¼•æ–‡ä»¶ï¼ˆWindows å…¼å®¹æ–¹å¼ï¼‰
                this.debugLog(`Replacing old index file`);
                try {
                    if (await this.fileExists(indexPath)) {
                        await fs.unlink(indexPath);
                    }
                } catch (unlinkError) {
                    // âœ… å¿½ç•¥ENOENTé”™è¯¯ï¼ˆæ–‡ä»¶å·²ä¸å­˜åœ¨ï¼‰
                    if (unlinkError.code !== 'ENOENT') {
                        throw unlinkError;
                    }
                    this.debugLog(`Index file doesn't exist, skipping unlink`);
                }
                await fs.rename(tempIndexPath, indexPath);
                
                writeSuccess = true;
                this.debugLog(`Index and database update completed successfully`);
                
            } catch (writeError) {
                console.error(`[VectorDB] Write operation failed for "${diaryName}":`, {
                    error: writeError.message,
                    tempIndex: tempIndexPath
                });
                
                // âœ… ä¿®å¤ï¼šå›æ»šæ•°æ®åº“ï¼ˆå› ä¸ºç´¢å¼•æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼‰
                let rollbackSuccess = false;
                try {
                    console.log(`[VectorDB] Rolling back database changes for "${diaryName}"`);
                    this.storage.saveChunks(diaryName, originalChunkMap);
                    
                    // âœ… éªŒè¯å›æ»šæ˜¯å¦æˆåŠŸ
                    const verifyChunkMap = this.storage.getChunkMap(diaryName);
                    if (Object.keys(verifyChunkMap).length === Object.keys(originalChunkMap).length) {
                        console.log(`[VectorDB] âœ… Database rollback successful and verified`);
                        rollbackSuccess = true;
                    } else {
                        throw new Error(`Rollback verification failed: expected ${Object.keys(originalChunkMap).length}, got ${Object.keys(verifyChunkMap).length}`);
                    }
                } catch (dbRollbackError) {
                    console.error(`[VectorDB] âŒ CRITICAL: Database rollback failed for "${diaryName}":`, dbRollbackError.message);
                    console.error(`[VectorDB] âš ï¸ Data inconsistency detected! Marking for full rebuild.`);
                    
                    // âœ… å›æ»šå¤±è´¥çš„è¡¥æ•‘æªæ–½ï¼šæ ‡è®°éœ€è¦å®Œæ•´é‡å»º
                    try {
                        // åˆ é™¤æŸåçš„ç´¢å¼•æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        const safeFileNameBase = Buffer.from(diaryName, 'utf-8').toString('base64url');
                        const indexPath = path.join(VECTOR_STORE_PATH, `${safeFileNameBase}.bin`);
                        if (await this.fileExists(indexPath)) {
                            await fs.unlink(indexPath);
                            console.log(`[VectorDB] Deleted inconsistent index file`);
                        }
                        
                        // æ¸…é™¤å†…å­˜ç¼“å­˜
                        this.indices.delete(diaryName);
                        this.chunkMaps.delete(diaryName);
                        
                        // è®°å½•å¤±è´¥ï¼Œè§¦å‘åç»­é‡å»º
                        this.storage.recordFailedRebuild(diaryName, `Data inconsistency after rollback failure: ${dbRollbackError.message}`);
                        
                        console.error(`[VectorDB] âš ï¸ "${diaryName}" marked for full rebuild due to data inconsistency`);
                    } catch (recoveryError) {
                        console.error(`[VectorDB] âŒ Recovery attempt also failed:`, recoveryError.message);
                    }
                }
                
                // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try {
                    if (await this.fileExists(tempIndexPath)) {
                        await fs.unlink(tempIndexPath);
                        this.debugLog(`Cleaned up temp index file`);
                    }
                } catch (cleanupError) {
                    console.warn(`[VectorDB] Failed to cleanup temp file:`, cleanupError.message);
                }
                
                // âœ… å¦‚æœå›æ»šæˆåŠŸï¼ŒæŠ›å‡ºåŸå§‹é”™è¯¯ï¼›å¦‚æœå¤±è´¥ï¼ŒæŠ›å‡ºæ›´ä¸¥é‡çš„é”™è¯¯
                if (rollbackSuccess) {
                    throw writeError;
                } else {
                    throw new Error(`CRITICAL: Write failed AND rollback failed for "${diaryName}". Full rebuild required. Original error: ${writeError.message}`);
                }
            }
            
            // âœ… åªæœ‰åœ¨å†™å…¥æˆåŠŸåæ‰æ¸…ç†å¤‡ä»½
            if (writeSuccess && hasBackup) {
                try {
                    if (await this.fileExists(backupIndexPath)) {
                        await fs.unlink(backupIndexPath);
                    }
                    this.debugLog(`Cleaned up backup file`);
                } catch (e) {
                    console.warn(`[VectorDB] Failed to cleanup backup file:`, e.message);
                }
            }

            // ç¬¬å…­é˜¶æ®µï¼šæ›´æ–°æ–‡ä»¶å“ˆå¸Œ
            this.storage.updateFileHashes(diaryName, newFileHashes);

            // âœ… æ›´æ–°å†…å­˜ä¸­çš„å¼•ç”¨
            this.chunkMaps.set(diaryName, chunkMap);

            this.stats.lastUpdateTime = new Date().toISOString();
            console.log(`[VectorDB] Incremental update for "${diaryName}" completed successfully.`);
            
        } catch (error) {
            console.error(`[VectorDB] Critical error during changeset application for "${diaryName}":`, error);
            
            // âœ… ä¿®å¤æ­»é”Bugï¼šä¸åœ¨é”å†…è°ƒç”¨rebuildIndexFromDatabase
            console.log(`[VectorDB] Will schedule atomic repair after releasing lock`);
            
            // âœ… æ ‡è®°éœ€è¦åŸå­ä¿®å¤ï¼Œåœ¨finallyå—é‡Šæ”¾é”åæ‰§è¡Œ
            shouldTriggerFullRebuild = true;
        } finally {
            this.releaseLock(diaryName); // âœ… å…ˆé‡Šæ”¾é”
        }
        
        // âœ… Bugä¿®å¤ï¼šåœ¨é”å¤–æ‰§è¡Œä¿®å¤ï¼Œé¿å…æ­»é”
        if (shouldTriggerFullRebuild) {
            console.log(`[VectorDB] Attempting atomic repair for "${diaryName}" after failed incremental update.`);
            
            // âœ… å…ˆå°è¯•åŸå­ä¿®å¤ï¼ˆä»æ•°æ®åº“é‡å»ºç´¢å¼•ï¼‰
            try {
                await this.rebuildIndexFromDatabase(diaryName);
                console.log(`[VectorDB] Successfully rebuilt index from database for "${diaryName}"`);
            } catch (rebuildError) {
                console.error(`[VectorDB] Atomic repair failed for "${diaryName}":`, rebuildError);
                console.log(`[VectorDB] Scheduling full rebuild worker as fallback`);
                
                // âœ… åŸå­ä¿®å¤å¤±è´¥ï¼Œå¯åŠ¨Workerè¿›è¡Œå®Œæ•´é‡å»º
                const workerStarted = this.runFullRebuildWorker(diaryName);
                if (!workerStarted) {
                    // Workerå¯åŠ¨å¤±è´¥ï¼Œéœ€è¦æ¸…ç†activeWorkers
                    this.activeWorkers.delete(diaryName);
                    console.log(`[VectorDB] Worker failed to start, cleared activeWorker for "${diaryName}"`);
                }
            }
        }
    }

    /**
     * ç»Ÿä¸€æœç´¢å…¥å£
     * @param {string} diaryName - æ—¥è®°æœ¬åç§°
     * @param {Array} queryVector - æŸ¥è¯¢å‘é‡
     * @param {number} k - è¿”å›ç»“æœæ•°é‡
     * @param {number|null} tagWeight - Tagæƒé‡ (0-1ä¹‹é—´)ï¼Œnullè¡¨ç¤ºä¸å¯ç”¨Tagæ£€ç´¢
     * @returns {Array} - æœç´¢ç»“æœ
     */
    async search(diaryName, queryVector, k = 3, tagWeight = null) {
        // âœ… å¦‚æœä¼ å…¥äº†tagWeightå‚æ•°ï¼Œä½¿ç”¨Tagå¢å¼ºæœç´¢
        if (tagWeight !== null && tagWeight !== undefined) {
            return await this.searchWithTagBoost(diaryName, queryVector, k, tagWeight);
        }

        // å¦åˆ™ä½¿ç”¨æ™®é€šå‘é‡æœç´¢
        const startTime = performance.now();
        const cached = this.searchCache.get(diaryName, queryVector, k);
        if (cached) {
            console.log(`[VectorDB][Search] Cache hit for "${diaryName}"`);
            this.recordMetric('search_success', performance.now() - startTime);
            return cached;
        }

        console.log(`[VectorDB][Search] Received async search request for "${diaryName}".`);
        await this.trackUsage(diaryName);

        // --- Bug Fix: K-value Sanity Check ---
        // 1. Ensure the index is loaded to check its size.
        const loaded = await this.loadIndexForSearch(diaryName);
        if (!loaded) {
            console.error(`[VectorDB][Search] Index for "${diaryName}" could not be loaded. Aborting search.`);
            return [];
        }

        const index = this.indices.get(diaryName);
        const maxElements = index.getMaxElements();

        // 2. If the index is empty, no search is possible.
        if (maxElements === 0) {
            console.log(`[VectorDB][Search] Index for "${diaryName}" is empty. Returning no results.`);
            return [];
        }

        // 3. Clamp the k-value to be no larger than the number of elements in the index.
        const finalK = Math.min(k, maxElements);
        if (finalK !== k) {
            console.warn(`[VectorDB][Search] Requested k=${k} is greater than max elements (${maxElements}) for "${diaryName}". Clamping to k=${finalK}.`);
        }
        // --- End of Bug Fix ---

        try {
            console.log(`[VectorDB][Search] Queuing search task for "${diaryName}" in worker pool.`);
            const workerData = {
                diaryName,
                queryVector,
                k: finalK, // Use the sanitized k-value
                efSearch: this.config.efSearch,
                vectorStorePath: VECTOR_STORE_PATH,
            };
            
            const message = await this.searchWorkerPool.execute(workerData);
            
            console.log(`[VectorDB][Search] Received message from worker for "${diaryName}". Status: ${message.status}`);
            if (message.status === 'success') {
                const searchResults = message.results;
                this.searchCache.set(diaryName, queryVector, k, searchResults); // Cache with original k
                this.recordMetric('search_success', performance.now() - startTime);
                console.log(`[VectorDB][Search] Worker found ${searchResults.length} matching chunks for "${diaryName}".`);
                return searchResults;
            } else {
                console.error(`[VectorDB][Search] Worker returned an error for "${diaryName}":`, message.error);
                return [];
            }
        } catch (error) {
            console.error(`[VectorDB][Search] Worker pool task for "${diaryName}" encountered a critical error:`, error);
            return [];
        }
    }

    /**
     * ğŸŒŸ å¸¦Tagæƒé‡çš„æœç´¢æ–¹æ³•ï¼ˆå‘é‡èåˆç‰ˆ - ä¸ä¾èµ–chunkçš„tagæ ‡è®°ï¼‰
     * @param {string} diaryName - æ—¥è®°æœ¬åç§°
     * @param {Array} queryVector - æŸ¥è¯¢å‘é‡
     * @param {number} k - è¿”å›ç»“æœæ•°é‡
     * @param {number} tagWeight - Tagæƒé‡/alpha (0-1ä¹‹é—´ï¼Œé»˜è®¤0.65)
     * @returns {Array} - æœç´¢ç»“æœ
     */
    async searchWithTagBoost(diaryName, queryVector, k = 3, tagWeight = 0.65) {
        const startTime = performance.now();
        
        // ğŸŒŸ æ£€æŸ¥Tag RAGç³»ç»Ÿå¼€å…³
        if (!this.tagRAGSystemEnabled) {
            console.log(`[VectorDB][TagSearch] Tag RAG System disabled, fallback to normal search`);
            return await this.search(diaryName, queryVector, k);
        }
        
        // å¦‚æœTagåŠŸèƒ½æœªå¯ç”¨ï¼Œå›é€€åˆ°æ™®é€šæœç´¢
        if (!this.tagVectorEnabled || !this.tagVectorManager) {
            console.log(`[VectorDB][TagSearch] Tag search disabled, fallback to normal search`);
            return await this.search(diaryName, queryVector, k);
        }

        // Tagå¢å¼ºæœç´¢å¼€å§‹ï¼ˆé™é»˜ï¼‰

        try {
            // Step 1: Tagå±‚ - è·å–è¯­ä¹‰ç›¸å…³çš„tagsåŠå…¶å‘é‡
            // âœ… ä¼˜åŒ–ï¼šå¢åŠ Tagå¬å›æ•°é‡ï¼Œå½¢æˆæ›´å¯†é›†çš„çŸ¥è¯†ç½‘ç»œ
            // æµ‹è¯•èŒƒå›´ï¼š50-100ä¸ªTagï¼ˆä»åŸæ¥çš„10-20ä¸ªæå‡ï¼‰
            const baseTagCount = 50;  // åŸºç¡€å¬å›æ•°é‡
            const scaledTagCount = k * 10;  // æ ¹æ®kå€¼åŠ¨æ€ç¼©æ”¾
            const topTagCount = Math.min(Math.max(baseTagCount, scaledTagCount), 100);  // ä¸Šé™100
            
            // Tagå¬å›ï¼ˆé™é»˜ï¼‰
            const matchedTags = await this.tagVectorManager.searchSimilarTags(queryVector, topTagCount);
            
            if (matchedTags.length === 0) {
                console.log(`[VectorDB][TagSearch] No matched tags, fallback to normal search`);
                return await this.search(diaryName, queryVector, k);
            }

            // åŒ¹é…åˆ°tagsï¼ˆé™é»˜ï¼‰

            // ğŸŒŸ Step 1.5: Tagå›¾æ‰©å±• - ä½¿ç”¨å…±ç°ç½‘ç»œæ‰©å±•ç›¸å…³tags
            let expandedTags = matchedTags;
            if (this.tagExpanderEnabled && this.tagExpander) {
                try {
                    const seedTags = matchedTags.slice(0, 20).map(t => t.tag); // å–å‰20ä¸ªä½œä¸ºç§å­
                    const maxExpansion = parseInt(process.env.TAG_EXPAND_MAX_COUNT) || 30;
                    const minWeight = parseInt(process.env.TAG_EXPAND_MIN_WEIGHT) || 2;
                    
                    // Tagå›¾æ‰©å±•ï¼ˆé™é»˜ï¼‰
                    const graphExpanded = await this.tagExpander.expandTags(seedTags, maxExpansion);
                    
                    if (graphExpanded.length > 0) {
                        // å›¾æ‰©å±•æ‰¾åˆ°ç›¸å…³tagsï¼ˆé™é»˜ï¼‰
                        
                        // ğŸŒŸ åˆå¹¶å‘é‡åŒ¹é…çš„tagså’Œå›¾æ‰©å±•çš„tags
                        // è¿‡æ»¤æ‰æƒé‡è¿‡ä½çš„æ‰©å±•tags
                        const validExpanded = graphExpanded.filter(t => t.weight >= minWeight);
                        
                        // ä¸ºå›¾æ‰©å±•çš„tagsè·å–å‘é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        const expandedWithVectors = [];
                        for (const expTag of validExpanded) {
                            const tagData = this.tagVectorManager.globalTags.get(expTag.tag);
                            if (tagData && tagData.vector) {
                                // å½’ä¸€åŒ–æƒé‡åˆ°0-1èŒƒå›´ï¼ˆå‡è®¾æƒé‡èŒƒå›´æ˜¯2-50ï¼‰
                                const normalizedScore = Math.min(expTag.weight / 50, 1.0);
                                expandedWithVectors.push({
                                    tag: expTag.tag,
                                    score: normalizedScore * 0.8, // é™ä½æ‰©å±•tagsçš„æƒé‡ï¼ˆç›¸æ¯”å‘é‡åŒ¹é…ï¼‰
                                    frequency: tagData.frequency,
                                    diaryCount: tagData.diaries.size,
                                    diaries: Array.from(tagData.diaries),
                                    isExpanded: true, // æ ‡è®°ä¸ºæ‰©å±•tag
                                    cooccurrenceWeight: expTag.weight
                                });
                            }
                        }
                        
                        // å·²æ·»åŠ å›¾æ‰©å±•tagsï¼ˆé™é»˜ï¼‰
                        
                        // åˆå¹¶åŸå§‹åŒ¹é…tagså’Œæ‰©å±•tagsï¼ˆå»é‡ï¼‰
                        const allTagsMap = new Map();
                        matchedTags.forEach(t => allTagsMap.set(t.tag, t));
                        expandedWithVectors.forEach(t => {
                            if (!allTagsMap.has(t.tag)) {
                                allTagsMap.set(t.tag, t);
                            }
                        });
                        
                        expandedTags = Array.from(allTagsMap.values());
                        // Tagæ‰©å±•å®Œæˆï¼ˆé™é»˜ï¼‰
                    } else {
                        // æ— é¢å¤–æ‰©å±•ï¼ˆé™é»˜ï¼‰
                    }
                } catch (expandError) {
                    console.error(`[VectorDB][TagSearch] Graph expansion failed:`, expandError.message);
                    // ç»§ç»­ä½¿ç”¨åŸå§‹matchedTags
                }
            }

            // Step 2: å‘é‡èåˆ - æ„å»ºTagå¢å¼ºçš„æŸ¥è¯¢å‘é‡ï¼ˆä½¿ç”¨æ‰©å±•åçš„tagsï¼‰
            // âœ… è§£è€¦ï¼šé€šè¿‡æ–°æ¥å£æ‰¹é‡è·å–å‘é‡ï¼Œä¸å†ç›´æ¥è®¿é—® globalTags
            const tagNames = expandedTags.map(t => t.tag);
            const retrievedVectors = await this.tagVectorManager.getVectorsForTags(tagNames);

            const tagVectors = [];
            const tagWeights = [];

            for (let i = 0; i < expandedTags.length; i++) {
                const vector = retrievedVectors[i];
                if (vector) {
                    tagVectors.push(vector);
                    tagWeights.push(expandedTags[i].score); // ä½¿ç”¨åŸå§‹çš„scoreä½œä¸ºæƒé‡
                }
            }

            if (tagVectors.length === 0) {
                console.warn(`[VectorDB][TagSearch] No tag vectors available after expansion, fallback`);
                return await this.search(diaryName, queryVector, k);
            }

            // Tagå‘é‡èåˆï¼ˆé™é»˜ï¼‰

            // è®¡ç®—tagå‘é‡çš„åŠ æƒå¹³å‡
            const dimensions = queryVector.length;
            const avgTagVector = new Array(dimensions).fill(0);
            let totalWeight = 0;

            for (let i = 0; i < tagVectors.length; i++) {
                const weight = tagWeights[i];
                totalWeight += weight;
                for (let j = 0; j < dimensions; j++) {
                    avgTagVector[j] += tagVectors[i][j] * weight;
                }
            }

            // å½’ä¸€åŒ–
            if (totalWeight > 0) {
                for (let j = 0; j < dimensions; j++) {
                    avgTagVector[j] /= totalWeight;
                }
            }

            // ğŸŒŸ æ ¸å¿ƒèåˆå…¬å¼ï¼šenhancedQuery = (1-Î±)Ã—query + Î±Ã—tagAvg
            const enhancedQueryVector = new Array(dimensions);
            for (let i = 0; i < dimensions; i++) {
                enhancedQueryVector[i] = (1 - tagWeight) * queryVector[i] + tagWeight * avgTagVector[i];
            }

            // æŸ¥è¯¢å‘é‡å·²å¢å¼ºï¼ˆé™é»˜ï¼‰

            // Step 3: ä½¿ç”¨å¢å¼ºåçš„å‘é‡æœç´¢
            const searchResults = await this.search(diaryName, enhancedQueryVector, k);

            console.log(`[VectorDB][TagSearch] Tagå¢å¼ºæœç´¢å®Œæˆ: ${searchResults.length}æ¡ç»“æœ (${(performance.now() - startTime).toFixed(0)}ms, ${expandedTags.length}tags)`);

            // âœ… åœ¨ç»“æœä¸­é™„åŠ tagä¿¡æ¯ï¼ˆåŒ…å«å›¾æ‰©å±•ä¿¡æ¯ï¼‰
            const enhancedResults = searchResults.map(result => {
                // è®¡ç®—TagåŒ¹é…åˆ†æ•°ï¼ˆå½’ä¸€åŒ–çš„å¹³å‡ç›¸ä¼¼åº¦ï¼‰
                const avgTagScore = expandedTags.length > 0
                    ? expandedTags.reduce((sum, t) => sum + t.score, 0) / expandedTags.length
                    : 0;
                
                // è®¡ç®—ææƒå€æ•°ï¼šåŸºäºTagæƒé‡å’ŒåŒ¹é…æ•°é‡
                const boostFactor = 1 + (tagWeight * avgTagScore * Math.min(expandedTags.length, 10) / 10);
                
                // åˆ†ç¦»å‘é‡åŒ¹é…çš„tagså’Œå›¾æ‰©å±•çš„tags
                const vectorMatchedTags = expandedTags.filter(t => !t.isExpanded);
                const graphExpandedTags = expandedTags.filter(t => t.isExpanded);
                
                return {
                    ...result,
                    // âœ… ä¿ç•™åŸå§‹å¾—åˆ†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    originalScore: result.score,
                    // âœ… Tagå¢å¼ºåçš„å¾—åˆ†
                    score: result.score * boostFactor,
                    // âœ… TagåŒ¹é…ä¿¡æ¯
                    tagMatchScore: avgTagScore,
                    matchedTags: vectorMatchedTags.slice(0, 10).map(t => t.tag), // å‘é‡åŒ¹é…çš„tags
                    expandedTags: graphExpandedTags.slice(0, 10).map(t => ({ // ğŸŒŸ å›¾æ‰©å±•çš„tags
                        tag: t.tag,
                        weight: t.cooccurrenceWeight
                    })),
                    tagMatchCount: vectorMatchedTags.length,
                    expandedTagCount: graphExpandedTags.length, // ğŸŒŸ æ‰©å±•tagæ•°é‡
                    totalTagCount: expandedTags.length,
                    boostFactor: boostFactor
                };
            });

            this.recordMetric('search_success', performance.now() - startTime);
            return enhancedResults;

        } catch (error) {
            console.error(`[VectorDB][TagSearch] Tag-enhanced search failed:`, error);
            console.log(`[VectorDB][TagSearch] Fallback to normal search`);
            return await this.search(diaryName, queryVector, k);
        }
    }

    /**
     * æ ¹æ®æ–‡æœ¬å†…å®¹è·å–å¯¹åº”çš„å‘é‡
     * @param {string} diaryName - æ—¥è®°æœ¬åç§°
     * @param {string} text - è¦æŸ¥æ‰¾çš„æ–‡æœ¬
     * @returns {Array|null} - è¿”å›å¯¹åº”çš„å‘é‡ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›null
     */
    async getVectorByText(diaryName, text) {
        try {
            // ç¡®ä¿ç´¢å¼•å·²åŠ è½½
            const loaded = await this.loadIndexForSearch(diaryName);
            if (!loaded) {
                console.error(`[VectorDB][getVectorByText] Failed to load index for "${diaryName}"`);
                return null;
            }

            const index = this.indices.get(diaryName);
            const chunkMap = this.chunkMaps.get(diaryName);

            if (!index || !chunkMap) {
                console.error(`[VectorDB][getVectorByText] Index or chunkMap not found for "${diaryName}"`);
                return null;
            }

            // åœ¨ chunkMap ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡æœ¬
            const trimmedText = text.trim();
            for (const [label, data] of Object.entries(chunkMap)) {
                if (data.text.trim() === trimmedText) {
                    // æ‰¾åˆ°åŒ¹é…çš„æ–‡æœ¬ï¼Œä»ç´¢å¼•ä¸­è·å–å‘é‡
                    try {
                        const vector = index.getPoint(Number(label));
                        return vector;
                    } catch (error) {
                        console.error(`[VectorDB][getVectorByText] Error getting vector for label ${label}:`, error.message);
                        return null;
                    }
                }
            }

            console.warn(`[VectorDB][getVectorByText] Text not found in chunkMap for "${diaryName}"`);
            return null;
        } catch (error) {
            console.error(`[VectorDB][getVectorByText] Error:`, error);
            return null;
        }
    }

    async manageMemory() {
        const memUsage = process.memoryUsage().heapUsed;
        if (memUsage > this.config.maxMemoryUsage) {
            console.log('[VectorDB] Memory threshold exceeded, evicting least recently used indices...');
            const entries = Array.from(this.lruCache.entries()).sort(([,a], [,b]) => a.lastAccessed - b.lastAccessed);
            for (const [diaryName] of entries) {
                if (process.memoryUsage().heapUsed < this.config.maxMemoryUsage * 0.8) break;
                
                // âœ… è·³è¿‡æ­£åœ¨å¤„ç†çš„ç´¢å¼•
                if (this.activeWorkers.has(diaryName)) {
                    console.log(`[VectorDB] Skipping "${diaryName}" - currently active`);
                    continue;
                }
                
                this.indices.delete(diaryName);
                this.chunkMaps.delete(diaryName);
                this.lruCache.delete(diaryName);
                console.log(`[VectorDB] Evicted index for "${diaryName}" from memory.`);
            }
        }
    }


    /**
     * åŒæ­¥å¹¶ç¼“å­˜æ‰€æœ‰æ—¥è®°æœ¬åç§°çš„å‘é‡ï¼ˆå¢é‡æ›´æ–°ï¼‰
     */
    async cacheDiaryNameVectors() {
        console.log('[VectorDB] Starting to sync diary book name vectors...');
        
        // âœ… åŠ é”é˜²æ­¢å¹¶å‘è°ƒç”¨
        const lockKey = 'diary_name_vectors';
        await this.acquireLock(lockKey);
        
        try {
            const diaryNameVectors = this.storage.loadDiaryNameVectors();

            const diaryBooks = await fs.readdir(DIARY_ROOT_PATH, { withFileTypes: true });
            const currentDiaryNames = new Set();
            for (const dirent of diaryBooks) {
                if (dirent.isDirectory() && !dirent.name.startsWith('å·²æ•´ç†') && dirent.name !== 'VCPè®ºå›') {
                    currentDiaryNames.add(dirent.name);
                }
            }

            const cachedNames = new Set(diaryNameVectors.keys());
            let hasChanges = false;

            // æ£€æŸ¥å·²åˆ é™¤çš„æ—¥è®°æœ¬
            for (const cachedName of cachedNames) {
                if (!currentDiaryNames.has(cachedName)) {
                    diaryNameVectors.delete(cachedName);
                    hasChanges = true;
                    console.log(`[VectorDB] Removed deleted diary "${cachedName}" from name vector cache.`);
                }
            }

            // æ£€æŸ¥æ–°å¢çš„æ—¥è®°æœ¬
            const namesToVectorize = [];
            for (const currentName of currentDiaryNames) {
                if (!cachedNames.has(currentName)) {
                    namesToVectorize.push(currentName);
                }
            }

            if (namesToVectorize.length > 0) {
                console.log(`[VectorDB] Found ${namesToVectorize.length} new diary books to vectorize.`);
                hasChanges = true;
                try {
                    const vectors = await this.getEmbeddingsWithRetry(namesToVectorize);
                    if (vectors.length !== namesToVectorize.length) {
                        throw new Error(`Vectorization count mismatch: expected ${namesToVectorize.length}, got ${vectors.length}`);
                    }
                    for (let i = 0; i < namesToVectorize.length; i++) {
                        diaryNameVectors.set(namesToVectorize[i], vectors[i]);
                    }
                } catch (error) {
                    console.error('[VectorDB] Failed to vectorize new diary book names:', error);
                    // å¦‚æœå‘é‡åŒ–å¤±è´¥ï¼Œåˆ™ä¸ä¿å­˜ï¼Œé¿å…å†™å…¥éƒ¨åˆ†çŠ¶æ€
                    return;
                }
            }

            if (hasChanges) {
                this.storage.saveDiaryNameVectors(diaryNameVectors);
                console.log(`[VectorDB] Diary name vector cache updated. Total entries: ${diaryNameVectors.size}.`);
            } else {
                console.log('[VectorDB] Diary name vector cache is up-to-date.');
            }
        } finally {
            this.releaseLock(lockKey);
        }
    }

    /**
     * è·å–ç¼“å­˜çš„æ—¥è®°æœ¬åç§°å‘é‡
     * @param {string} diaryName - æ—¥è®°æœ¬åç§°
     * @returns {Array|null} - å‘é‡æ•°ç»„æˆ–null
     */
    getDiaryNameVector(diaryName) {
        const vectors = this.storage.loadDiaryNameVectors();
        return vectors.get(diaryName) || null;
    }

    /**
     * âœ… æ‰¹é‡å†™å…¥ä¼˜åŒ–ï¼šè®°å½•ä½¿ç”¨ç»Ÿè®¡åˆ°å†…å­˜ç¼“å†²åŒº
     * ä½¿ç”¨é˜²æŠ–æœºåˆ¶ï¼Œå‡å°‘ç£ç›˜å†™å…¥é¢‘ç‡
     */
    trackUsage(diaryName) {
        // æ›´æ–°å†…å­˜ç¼“å†²åŒºï¼ˆæ— éœ€å¼‚æ­¥æ“ä½œï¼Œæ€§èƒ½æ›´å¥½ï¼‰
        const current = this.usageStatsBuffer.get(diaryName) || {
            frequency: 0,
            lastAccessed: null
        };
        current.frequency++;
        current.lastAccessed = Date.now();
        this.usageStatsBuffer.set(diaryName, current);
        
        // é˜²æŠ–ï¼šé‡ç½®å®šæ—¶å™¨
        if (this.usageStatsFlushTimer) {
            clearTimeout(this.usageStatsFlushTimer);
        }
        
        // è®¾ç½®æ–°çš„å»¶è¿Ÿå†™å…¥ä»»åŠ¡
        this.usageStatsFlushTimer = setTimeout(() => {
            this.flushUsageStats().catch(e => {
                console.error('[VectorDB] Failed to flush usage stats:', e);
            });
        }, this.usageStatsFlushDelay);
    }
    
    /**
     * âœ… å°†ç¼“å†²åŒºæ•°æ®æ‰¹é‡å†™å…¥ç£ç›˜
     */
    async flushUsageStats() {
        if (this.usageStatsBuffer.size === 0) {
            this.debugLog('Usage stats buffer is empty, skipping flush');
            return;
        }
        
        const lockKey = 'usage_stats';
        let bufferSnapshot; // âœ… åœ¨å¤–éƒ¨ä½œç”¨åŸŸå®šä¹‰ï¼Œé¿å…catchå—ä¸­æœªå®šä¹‰
        let lockAcquired = false; // âœ… Bugä¿®å¤3ï¼šè¿½è¸ªé”çŠ¶æ€
        
        try {
            await this.acquireLock(lockKey);
            lockAcquired = true; // âœ… æ ‡è®°é”å·²è·å–
            
            bufferSnapshot = new Map(this.usageStatsBuffer);
            this.usageStatsBuffer.clear();
            
            // âœ… ç›´æ¥å†™å…¥SQLite
            this.storage.updateUsageStats(bufferSnapshot);
            
            this.debugLog(`Flushed ${bufferSnapshot.size} usage stats to database`);
            
        } catch (e) {
            console.error('[VectorDB] Failed to flush usage stats:', e.message);
            
            // å†™å…¥å¤±è´¥æ—¶æ”¾å›ç¼“å†²åŒº
            if (bufferSnapshot) { // âœ… æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                for (const [diaryName, data] of bufferSnapshot.entries()) {
                const existing = this.usageStatsBuffer.get(diaryName);
                if (existing) {
                    existing.frequency += data.frequency;
                    if (data.lastAccessed > existing.lastAccessed) {
                        existing.lastAccessed = data.lastAccessed;
                    }
                } else {
                    this.usageStatsBuffer.set(diaryName, data);
                }
            }
            }
            
            if (!this.isShuttingDown) {
                setTimeout(() => {
                    this.flushUsageStats().catch(console.error);
                }, 10000);
            }
        } finally {
            if (lockAcquired) { // âœ… Bugä¿®å¤3ï¼šåªé‡Šæ”¾å·²æŒæœ‰çš„é”
                this.releaseLock(lockKey);
            }
        }
    }

    async preWarmIndices() {
        console.log('[VectorDB] Starting index pre-warming...');
        const usageStats = this.storage.loadUsageStats();
        const sortedDiaries = Object.entries(usageStats)
            .sort(([,a], [,b]) => b.frequency - a.frequency)
            .map(([name]) => name)
            .filter(name => !name.startsWith('å·²æ•´ç†') && name !== 'VCPè®ºå›'); // âœ… è¿‡æ»¤æ‰è¢«å¿½ç•¥çš„æ—¥è®°æœ¬
        
        const preLoadCount = Math.min(this.config.preWarmCount, sortedDiaries.length);
        if (preLoadCount === 0) {
            console.log('[VectorDB] No usage stats found for active diaries, skipping pre-warming.');
            return;
        }
        
        // âœ… ä½¿ç”¨ç¼“å­˜çš„ç»´åº¦ï¼ˆé‡‘æ ‡å‡†ï¼‰
        const preLoadPromises = sortedDiaries
            .slice(0, preLoadCount)
            .map(diaryName => this.loadIndexForSearch(diaryName, this.embeddingDimensions));
        
        await Promise.all(preLoadPromises);
        console.log(`[VectorDB] Pre-warmed ${preLoadCount} most frequently used indices.`);
    }

    async fileExists(filePath) {
        try {
            await fs.access(filePath);
            return true;
        } catch {
            return false;
        }
    }
    /**
     * âœ… æ–°å¢ï¼šæ™ºèƒ½åŒæ­¥ç´¢å¼•ä¸æ•°æ®åº“ï¼ˆåŸºäºå·®å¼‚diffä¿®å¤ï¼‰
     * åªå¤„ç†ç¼ºå¤±çš„chunkï¼Œä¸é‡æ–°embeddingå·²æœ‰æ•°æ®
     */
    async syncIndexWithDatabase(diaryName) {
        await this.acquireLock(diaryName);
        try {
            console.log(`[VectorDB] Syncing index with database for "${diaryName}"...`);
            
            const safeFileNameBase = Buffer.from(diaryName, 'utf-8').toString('base64url');
            const indexPath = path.join(VECTOR_STORE_PATH, `${safeFileNameBase}.bin`);
            
            // ä»æ•°æ®åº“åŠ è½½å®Œæ•´æ•°æ®
            const chunkMap = this.storage.getChunkMap(diaryName);
            const dbLabels = new Set(Object.keys(chunkMap).map(Number));
            
            if (dbLabels.size === 0) {
                console.warn(`[VectorDB] No chunks in database for "${diaryName}", skipping sync`);
                return;
            }
            
            // å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
            let index = null;
            let indexLabels = new Set();
            
            try {
                // è·å–dimensions
                const dummyText = Object.values(chunkMap)[0].text;
                const cleanedText = this.prepareTextForEmbedding(dummyText);
                const dummyEmbedding = await this.getEmbeddingsWithRetry([cleanedText]);
                const dimensions = dummyEmbedding[0].length;
                
                index = new HierarchicalNSW('l2', dimensions);
                index.readIndexSync(indexPath);
                
                // æ”¶é›†ç´¢å¼•ä¸­çš„labels
                for (const label of dbLabels) {
                    try {
                        index.getPoint(label);
                        indexLabels.add(label);
                    } catch (e) {
                        // labelä¸å­˜åœ¨
                    }
                }
                
                console.log(`[VectorDB] Index has ${indexLabels.size}/${dbLabels.size} chunks from database`);
            } catch (e) {
                console.log(`[VectorDB] Cannot load index, will create new one`);
            }
            
            // è®¡ç®—éœ€è¦æ·»åŠ çš„chunksï¼ˆåœ¨æ•°æ®åº“ä¸­ä½†ä¸åœ¨ç´¢å¼•ä¸­ï¼‰
            const missingLabels = Array.from(dbLabels).filter(l => !indexLabels.has(l));
            
            if (missingLabels.length === 0 && indexLabels.size === dbLabels.size) {
                console.log(`[VectorDB] Index and database are already in sync for "${diaryName}"`);
                return;
            }
            
            console.log(`[VectorDB] Need to add ${missingLabels.length} missing chunks to index`);
            
            // åªå¯¹ç¼ºå¤±çš„chunksè·å–embedding
            const missingChunks = missingLabels.map(label => {
                const data = chunkMap[label];
                return this.prepareTextForEmbedding(data.text);
            });
            
            const vectors = await this.getEmbeddingsWithRetry(missingChunks);
            
            if (vectors.length !== missingLabels.length) {
                throw new Error(`Embedding count mismatch: expected ${missingLabels.length}, got ${vectors.length}`);
            }
            
            // å¦‚æœæ²¡æœ‰ç´¢å¼•ï¼Œåˆ›å»ºæ–°çš„
            if (!index) {
                const dimensions = vectors[0].length;
                index = new HierarchicalNSW('l2', dimensions);
                index.initIndex(dbLabels.size);
                
                // å…ˆæ·»åŠ æ‰€æœ‰å·²å­˜åœ¨çš„chunksï¼ˆéœ€è¦é‡æ–°embeddingï¼‰
                console.log(`[VectorDB] Creating new index with ${dbLabels.size} chunks...`);
                const allTexts = Array.from(dbLabels).map(label => 
                    this.prepareTextForEmbedding(chunkMap[label].text)
                );
                const allVectors = await this.getEmbeddingsWithRetry(allTexts);
                
                const sortedLabels = Array.from(dbLabels).sort((a, b) => a - b);
                for (let i = 0; i < allVectors.length; i++) {
                    index.addPoint(allVectors[i], sortedLabels[i]);
                }
            } else {
                // åªæ·»åŠ ç¼ºå¤±çš„chunks
                for (let i = 0; i < missingLabels.length; i++) {
                    try {
                        index.addPoint(vectors[i], missingLabels[i]);
                    } catch (e) {
                        console.error(`[VectorDB] Failed to add point ${missingLabels[i]}:`, e.message);
                    }
                }
            }
            
            // ä¿å­˜ç´¢å¼•æ–‡ä»¶
            await index.writeIndex(indexPath);
            
            // æ›´æ–°å†…å­˜
            this.indices.set(diaryName, index);
            this.chunkMaps.set(diaryName, chunkMap);
            
            console.log(`[VectorDB] âœ… Index synced with database for "${diaryName}" (${dbLabels.size} chunks, added ${missingLabels.length} missing)`);
        } catch (error) {
            console.error(`[VectorDB] Failed to sync index for "${diaryName}":`, error);
            throw error;
        } finally {
            this.releaseLock(diaryName);
        }
    }


    /**
     * âœ… æ–°å¢ï¼šä»æ•°æ®åº“é‡å»ºç´¢å¼•ï¼ˆåŸå­åŒ–ä¿®å¤ï¼Œæ— éœ€é‡æ–°æ‰«ææ–‡ä»¶ï¼‰
     */
    async rebuildIndexFromDatabase(diaryName) {
        await this.acquireLock(diaryName);
        try {
            console.log(`[VectorDB] Rebuilding index from database for "${diaryName}"...`);
            
            const chunkMap = this.storage.getChunkMap(diaryName);
            const chunks = Object.values(chunkMap).map(data => data.text);
            
            if (chunks.length === 0) {
                console.warn(`[VectorDB] No chunks in database for "${diaryName}", skipping rebuild`);
                return;
            }
            
            // æ¸…ç†åçš„æ–‡æœ¬ç”¨äºembedding
            const cleanedTexts = chunks.map(text => this.prepareTextForEmbedding(text));
            
            // è·å–embeddings
            console.log(`[VectorDB] Getting embeddings for ${chunks.length} chunks from database...`);
            const vectors = await this.getEmbeddingsWithRetry(cleanedTexts);
            
            if (vectors.length !== chunks.length) {
                throw new Error(`Embedding count mismatch: expected ${chunks.length}, got ${vectors.length}`);
            }
            
            // åˆ›å»ºæ–°ç´¢å¼•
            const dimensions = vectors[0].length;
            const index = new HierarchicalNSW('l2', dimensions);
            index.initIndex(chunks.length);
            
            // æ·»åŠ æ‰€æœ‰å‘é‡
            const labels = Object.keys(chunkMap).map(Number).sort((a, b) => a - b);
            for (let i = 0; i < vectors.length; i++) {
                index.addPoint(vectors[i], labels[i]);
            }
            
            // ä¿å­˜ç´¢å¼•æ–‡ä»¶
            const safeFileNameBase = Buffer.from(diaryName, 'utf-8').toString('base64url');
            const indexPath = path.join(VECTOR_STORE_PATH, `${safeFileNameBase}.bin`);
            await index.writeIndex(indexPath);
            
            // æ›´æ–°å†…å­˜
            this.indices.set(diaryName, index);
            this.chunkMaps.set(diaryName, chunkMap);
            
            console.log(`[VectorDB] âœ… Index rebuilt from database for "${diaryName}" (${chunks.length} vectors)`);
        } catch (error) {
            console.error(`[VectorDB] Failed to rebuild index from database for "${diaryName}":`, error);
            throw error;
        } finally {
            this.releaseLock(diaryName);
        }
    }

    async shutdown() {
        console.log('[VectorDB] Shutting down...');
        this.isShuttingDown = true;
        
        // âœ… å–æ¶ˆå¾…å¤„ç†çš„flushå®šæ—¶å™¨
        if (this.usageStatsFlushTimer) {
            clearTimeout(this.usageStatsFlushTimer);
            this.usageStatsFlushTimer = null;
        }
        
        // âœ… ç«‹å³åˆ·æ–°ç¼“å†²åŒºæ•°æ®ï¼ˆç¡®ä¿æ•°æ®ä¸ä¸¢å¤±ï¼‰
        if (this.usageStatsBuffer.size > 0) {
            console.log('[VectorDB] Flushing usage stats buffer before shutdown...');
            try {
                await this.flushUsageStats();
                console.log('[VectorDB] Usage stats flushed successfully.');
            } catch (e) {
                console.error('[VectorDB] Failed to flush usage stats during shutdown:', e);
            }
        }
        
        // å…³é—­ worker pool
        if (this.searchWorkerPool && typeof this.searchWorkerPool.terminate === 'function') {
            await this.searchWorkerPool.terminate();
            console.log('[VectorDB] Worker pool shut down successfully.');
        } else {
            console.log('[VectorDB] Worker pool not found or does not have a terminate method.');
        }
        
        // å…³é—­æ•°æ®åº“è¿æ¥
        this.storage.close();
        
        console.log('[VectorDB] Shutdown complete.');
    }
}

// --- Standalone functions for Worker ---
async function getEmbeddingsInWorker(chunks, config) {
    const { default: fetch } = await import('node-fetch');
    const allVectors = [];
    const batchSize = parseInt(process.env.VECTORDB_BATCH_SIZE) || 5;

    const retryAttempts = config.retryAttempts || 3;
    const retryBaseDelay = config.retryBaseDelay || 1000;
    const retryMaxDelay = config.retryMaxDelay || 10000;

    for (let i = 0; i < chunks.length; i += batchSize) {
        const batch = chunks.slice(i, i + batchSize);
        
        let lastError;
        for (let attempt = 1; attempt <= retryAttempts; attempt++) {
            try {
                const response = await fetch(`${config.apiUrl}/v1/embeddings`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': `Bearer ${config.apiKey}`
                    },
                    body: JSON.stringify({
                        model: config.embeddingModel,
                        input: batch
                    })
                });

                if (!response.ok) {
                    const errorBody = await response.text();
                    const err = new Error(`Embedding API error: ${response.status} - ${errorBody}`);
                    err.status = response.status;
                    err.headers = response.headers; // âœ… ä¿å­˜headers
                    throw err;
                }

                const data = await response.json();
                
                if (!data.data || !Array.isArray(data.data)) {
                    throw new Error(`Invalid API response: missing data array`);
                }
                
                const vectors = data.data.map(item => item.embedding);
                
                for (let j = 0; j < vectors.length; j++) {
                    if (!vectors[j] || !Array.isArray(vectors[j]) || vectors[j].length === 0) {
                        throw new Error(`Invalid embedding at index ${j} for text: "${batch[j].substring(0, 50)}..."`);
                    }
                }
                
                if (vectors.length !== batch.length) {
                    throw new Error(`Batch size mismatch: sent ${batch.length}, received ${vectors.length}`);
                }
                
                allVectors.push(...vectors);
                
                lastError = null;
                break;
            } catch (error) {
                lastError = error;
                console.error(`[VectorDB][Worker] Batch attempt ${attempt}/${retryAttempts} failed:`, error.message);
                
                if (attempt < retryAttempts) {
                    let delay;
                    
                    if (error.status === 429) {
                        const retryAfter = error.headers?.get('retry-after');
                        if (retryAfter) {
                            if (/^\d+$/.test(retryAfter)) {
                                delay = parseInt(retryAfter) * 1000;
                            } else {
                                const retryDate = new Date(retryAfter);
                                delay = retryDate.getTime() - Date.now();
                            }
                            delay = Math.max(0, Math.min(delay, 60000)); // Wait at most 60s
                        } else {
                            delay = 30000; // Default 30s
                        }
                        console.log(`[VectorDB][Worker] Rate limited (429). Waiting ${delay}ms...`);
                    } else {
                        delay = Math.min(retryBaseDelay * Math.pow(2, attempt - 1), retryMaxDelay);
                    }
                    
                    await new Promise(resolve => setTimeout(resolve, delay));
                }
            }
        }

        if (lastError) {
            // âœ… åŒºåˆ†429é™æµé”™è¯¯å’Œå…¶ä»–é”™è¯¯
            if (lastError.status === 429) {
                console.warn(`[VectorDB][Worker] Rate limit hit after ${retryAttempts} attempts. Stopping batch processing to allow quota recovery.`);
                console.warn(`[VectorDB][Worker] Progress has been saved. Will resume on next run.`);
                // âœ… æŠ›å‡ºç‰¹æ®Šçš„é™æµé”™è¯¯ï¼Œè®©WorkerçŸ¥é“è¿™æ˜¯å¯æ¢å¤çš„
                const rateLimitError = new Error('RATE_LIMIT_EXCEEDED');
                rateLimitError.isRateLimitError = true;
                rateLimitError.processedCount = i; // è®°å½•å·²å¤„ç†çš„chunkæ•°é‡
                throw rateLimitError;
            } else {
                // å…¶ä»–é”™è¯¯æŒ‰åŸé€»è¾‘å¤„ç†
                throw new Error(
                    `Failed to embed batch (chunks ${i}-${i+batch.length-1}) after ${retryAttempts} attempts.\n` +
                    `Last error: ${lastError.message}\n` +
                    `Sample text: "${batch[0].substring(0, 100)}..."`
                );
            }
        }
    }
    return allVectors;
}

async function processSingleDiaryBookInWorker(diaryName, config) {
    const VectorDBStorage = require('./VectorDBStorage.js');
    
    // âœ… å®šä¹‰ emoji æ¸…ç†å‡½æ•°ï¼ˆä¸ä¸»ç±»ä¿æŒä¸€è‡´ï¼‰
    const prepareTextForEmbedding = (text) => {
        const decorativeEmojis = /[\u{1F600}-\u{1F64F}\u{1F300}-\u{1F5FF}\u{1F680}-\u{1F6FF}\u{1F1E0}-\u{1F1FF}\u{2600}-\u{26FF}\u{2700}-\u{27BF}]/gu;
        let cleaned = text.replace(decorativeEmojis, ' ').replace(/\s+/g, ' ').trim();
        return cleaned.length === 0 ? '[EMPTY_CONTENT]' : cleaned;
    };

    const diaryPath = path.join(DIARY_ROOT_PATH, diaryName);
    const files = await fs.readdir(diaryPath);
    const relevantFiles = files.filter(f => f.toLowerCase().endsWith('.txt') || f.toLowerCase().endsWith('.md'));

    // âœ… åˆå§‹åŒ–Storage
    const storage = new VectorDBStorage(VECTOR_STORE_PATH);
    await storage.initialize();

    try {
        // âœ… Step 1: æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„æ„å»ºè¿›åº¦
        const progress = storage.getBuildProgress(diaryName);
        const processedFiles = progress ? new Set(progress.processedFiles) : new Set();
        
        console.log(`[VectorDB][Worker] "${diaryName}" build progress: ${processedFiles.size}/${relevantFiles.length} files processed`);
        
        // âœ… Step 2: åŠ è½½å·²æœ‰çš„ç´¢å¼•å’ŒchunkMapï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        const safeFileNameBase = Buffer.from(diaryName, 'utf-8').toString('base64url');
        const indexPath = path.join(VECTOR_STORE_PATH, `${safeFileNameBase}.bin`);
        
        let index = null;
        let chunkMap = {};
        let labelCounter = 0;
        let dimensions = null;
        
        // å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
        try {
            await fs.access(indexPath);
            const existingChunkMap = storage.getChunkMap(diaryName);
            
            if (Object.keys(existingChunkMap).length > 0) {
                chunkMap = existingChunkMap;
                labelCounter = Math.max(...Object.keys(chunkMap).map(Number)) + 1;
                
                // è·å–dimensions
                const dummyText = Object.values(chunkMap)[0].text;
                const dummyEmbedding = await getEmbeddingsInWorker([prepareTextForEmbedding(dummyText)], config);
                dimensions = dummyEmbedding[0].length;
                
                // âœ… éªŒè¯ç»´åº¦
                if (config.expectedDimensions && dimensions !== config.expectedDimensions) {
                    throw new Error(`[VectorDB][Worker] Invalid vector dimension. Expected ${config.expectedDimensions}, but got ${dimensions}.`);
                }
                
                index = new HierarchicalNSW('l2', dimensions);
                index.readIndexSync(indexPath);
                
                console.log(`[VectorDB][Worker] Loaded existing index with ${Object.keys(chunkMap).length} chunks, continuing from label ${labelCounter}`);
            }
        } catch (e) {
            console.log(`[VectorDB][Worker] No existing index found, will create new one`);
        }

        // âœ… Step 3: æ¸è¿›å¼å¤„ç†æ–‡ä»¶ï¼ˆæŒ‰æ–‡ä»¶ä¸ºå•ä½ä¿å­˜ï¼‰
        const fileHashes = {};
        const SAVE_INTERVAL = parseInt(process.env.VECTORDB_SAVE_INTERVAL) || 10; // æ¯10ä¸ªæ–‡ä»¶ä¿å­˜ä¸€æ¬¡
        let filesProcessedSinceLastSave = 0;
        
        for (const file of relevantFiles) {
            // è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶
            if (processedFiles.has(file)) {
                console.log(`[VectorDB][Worker] Skipping already processed file: "${file}"`);
                const filePath = path.join(diaryPath, file);
                const content = await fs.readFile(filePath, 'utf-8');
                fileHashes[file] = crypto.createHash('md5').update(content).digest('hex');
                continue;
            }
            
            const filePath = path.join(diaryPath, file);
            const content = await fs.readFile(filePath, 'utf-8');
            fileHashes[file] = crypto.createHash('md5').update(content).digest('hex');
            
            const chunks = chunkText(content);
            const fileChunks = [];
            const fileChunkTexts = [];
            
            for (const chunk of chunks) {
                const cleanedText = prepareTextForEmbedding(chunk);
                if (cleanedText === '[EMPTY_CONTENT]') {
                    continue;
                }
                
                const chunkHash = crypto.createHash('sha256').update(chunk).digest('hex');
                fileChunks.push({ chunk, chunkHash });
                fileChunkTexts.push(cleanedText);
            }
            
            if (fileChunks.length === 0) {
                console.warn(`[VectorDB][Worker] File "${file}" has no valid chunks, skipping`);
                processedFiles.add(file);
                continue;
            }
            
            // âœ… è·å–è¿™ä¸ªæ–‡ä»¶çš„æ‰€æœ‰embeddings
            console.log(`[VectorDB][Worker] Processing file "${file}" (${fileChunks.length} chunks)...`);
            let fileVectors;
            try {
                fileVectors = await getEmbeddingsInWorker(fileChunkTexts, config);
                
                if (fileVectors.length !== fileChunks.length) {
                    throw new Error(`Embedding count mismatch for file "${file}": expected ${fileChunks.length}, got ${fileVectors.length}`);
                }
            } catch (error) {
                // âœ… æ ¸å¿ƒä¿®å¤ï¼šåœ¨å¤„ç†ä»»ä½•é”™è¯¯ä¹‹å‰ï¼Œå…ˆä¿å­˜å·²å®Œæˆçš„è¿›åº¦
                // è¿™ç¡®ä¿äº†å³ä½¿å‘ç”ŸOOMã€ç³»ç»Ÿkillã€ç½‘ç»œé”™è¯¯ç­‰å´©æºƒï¼Œå·²å¤„ç†çš„æ•°æ®ä¹Ÿä¸ä¼šä¸¢å¤±
                console.error(`[VectorDB][Worker] âŒ Error while processing "${file}":`, error.message);
                
                if (processedFiles.size > 0 && index) {
                    console.warn(`[VectorDB][Worker] ğŸ’¾ Saving progress before handling error...`);
                    try {
                        await index.writeIndex(indexPath);
                        storage.saveChunks(diaryName, chunkMap);
                        storage.saveBuildProgress(diaryName, Array.from(processedFiles), relevantFiles.length, Array.from(processedFiles).pop());
                        console.log(`[VectorDB][Worker] âœ… Progress saved: ${processedFiles.size}/${relevantFiles.length} files (${Object.keys(chunkMap).length} chunks)`);
                    } catch (saveError) {
                        console.error(`[VectorDB][Worker] âš ï¸ Failed to save progress:`, saveError.message);
                        // ç»§ç»­å¤„ç†åŸå§‹é”™è¯¯
                    }
                }
                
                // âœ… ç„¶åæ ¹æ®é”™è¯¯ç±»å‹å†³å®šå¦‚ä½•å¤„ç†
                if (error.isRateLimitError) {
                    // é™æµé”™è¯¯ï¼šä¼˜é›…æš‚åœ
                    console.warn(`[VectorDB][Worker] â¸ï¸ Rate limit encountered - progress already saved`);
                    console.warn(`[VectorDB][Worker] Next file to process: "${file}"`);
                    
                    storage.close();
                    
                    // âœ… æŠ›å‡ºç‰¹æ®Šé”™è¯¯è®©ManagerçŸ¥é“è¿™æ˜¯æ­£å¸¸çš„æš‚åœ
                    const pauseError = new Error(`Rate limit reached. Progress saved at ${processedFiles.size}/${relevantFiles.length} files. Will resume automatically on next run.`);
                    pauseError.isPauseError = true;
                    pauseError.processedFiles = processedFiles.size;
                    pauseError.totalFiles = relevantFiles.length;
                    throw pauseError;
                } else {
                    // å…¶ä»–é”™è¯¯ï¼šä¿å­˜åé‡æ–°æŠ›å‡º
                    console.error(`[VectorDB][Worker] âŒ Non-recoverable error. Progress has been saved, will retry from file "${file}" on next run.`);
                    throw error;
                }
            }
            
            // âœ… åˆå§‹åŒ–ç´¢å¼•ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
            if (!index) {
                dimensions = fileVectors[0].length;
                
                // âœ… éªŒè¯ç»´åº¦
                if (config.expectedDimensions && dimensions !== config.expectedDimensions) {
                    throw new Error(`[VectorDB][Worker] Invalid vector dimension. Expected ${config.expectedDimensions}, but got ${dimensions}.`);
                }
                
                index = new HierarchicalNSW('l2', dimensions);
                // âœ… ä¿®å¤ï¼šæ™ºèƒ½å®¹é‡é¢„ä¼°ï¼Œæ”¯æŒå¤§è§„æ¨¡ä¸“ä¸šè®ºæ–‡é›†
                const processedChunkCount = Object.keys(chunkMap).length;
                const processedFileCount = processedFiles.size;
                const remainingFiles = relevantFiles.length - processedFiles.size;
                
                let estimatedTotal;
                
                if (processedFileCount > 0) {
                    // åŸºäºå·²å¤„ç†æ–‡ä»¶åŠ¨æ€è®¡ç®—å¹³å‡å€¼
                    const avgChunksPerFile = Math.ceil(processedChunkCount / processedFileCount);
                    estimatedTotal = processedChunkCount + (remainingFiles * avgChunksPerFile);
                    console.log(`[VectorDB][Worker] Capacity estimation based on ${processedFileCount} processed files (avg: ${avgChunksPerFile} chunks/file)`);
                } else {
                    // é¦–æ¬¡åˆ›å»ºï¼šåŸºäºæ€»æ–‡ä»¶æ•°åšä¿å®ˆä¼°è®¡
                    // å‡è®¾æ¯ä¸ªæ–‡ä»¶è‡³å°‘200ä¸ªchunksï¼ˆé€‚é…å¤§å‹è®ºæ–‡é›†ï¼‰
                    const conservativeEstimate = relevantFiles.length * 200;
                    estimatedTotal = Math.max(conservativeEstimate, 5000);
                    console.log(`[VectorDB][Worker] Initial capacity estimation for ${relevantFiles.length} files: ${estimatedTotal} chunks`);
                }
                
                // âœ… å…³é”®ï¼šä¸ºå¤§è§„æ¨¡æ•°æ®é›†å¢åŠ 50%ç¼“å†²ï¼ˆè€Œé20%ï¼‰
                const capacityWithBuffer = Math.ceil(estimatedTotal * 1.5);
                const finalCapacity = Math.max(capacityWithBuffer, 10000); // æœ€å°å®¹é‡æå‡åˆ°10000
                
                index.initIndex(finalCapacity);
                console.log(`[VectorDB][Worker] âœ… Index initialized with capacity: ${finalCapacity.toLocaleString()} (estimated: ${estimatedTotal.toLocaleString()})`);
            }
            
            // âœ… æ·»åŠ è¿™ä¸ªæ–‡ä»¶çš„æ‰€æœ‰å‘é‡åˆ°ç´¢å¼•ï¼ˆå¸¦å®¹é‡æ£€æŸ¥ï¼‰
            for (let i = 0; i < fileVectors.length; i++) {
                const currentLabel = labelCounter++;
                
                // âœ… å…³é”®ä¿®å¤ï¼šæ·»åŠ å‰æ£€æŸ¥å®¹é‡ï¼Œå¿…è¦æ—¶æ‰©å®¹
                const currentCount = Object.keys(chunkMap).length + 1;
                const currentCapacity = index.getMaxElements();
                
                if (currentCount > currentCapacity) {
                    const newCapacity = Math.ceil(currentCapacity * 1.5);
                    console.log(`[VectorDB][Worker] âš ï¸ Capacity exceeded! Resizing from ${currentCapacity} to ${newCapacity}`);
                    index.resizeIndex(newCapacity);
                }
                
                index.addPoint(fileVectors[i], currentLabel);
                chunkMap[currentLabel] = {
                    text: fileChunks[i].chunk,
                    sourceFile: file,
                    chunkHash: fileChunks[i].chunkHash
                };
            }
            
            processedFiles.add(file);
            filesProcessedSinceLastSave++;
            
            // âœ… Step 4: å®šæœŸä¿å­˜è¿›åº¦ï¼ˆæ¯Nä¸ªæ–‡ä»¶æˆ–æœ€åä¸€ä¸ªæ–‡ä»¶ï¼‰
            const isLastFile = processedFiles.size === relevantFiles.length;
            const shouldSave = filesProcessedSinceLastSave >= SAVE_INTERVAL || isLastFile;
            
            if (shouldSave) {
                console.log(`[VectorDB][Worker] Saving progress: ${processedFiles.size}/${relevantFiles.length} files (${Object.keys(chunkMap).length} chunks)...`);
                
                // ä¿å­˜ç´¢å¼•æ–‡ä»¶
                await index.writeIndex(indexPath);
                
                // ä¿å­˜chunkMapåˆ°æ•°æ®åº“
                storage.saveChunks(diaryName, chunkMap);
                
                // éªŒè¯ä¿å­˜
                const savedChunkMap = storage.getChunkMap(diaryName);
                if (Object.keys(savedChunkMap).length !== Object.keys(chunkMap).length) {
                    throw new Error(`Checkpoint save verification failed: expected ${Object.keys(chunkMap).length}, got ${Object.keys(savedChunkMap).length}`);
                }
                
                // ä¿å­˜è¿›åº¦
                if (!isLastFile) {
                    storage.saveBuildProgress(diaryName, Array.from(processedFiles), relevantFiles.length, file);
                    console.log(`[VectorDB][Worker] âœ… Checkpoint saved: ${processedFiles.size}/${relevantFiles.length} files`);
                }
                
                filesProcessedSinceLastSave = 0;
            }
        }

        // âœ… Step 5: æœ€ç»ˆéªŒè¯
        if (Object.keys(chunkMap).length === 0) {
            throw new Error(`Diary "${diaryName}" resulted in 0 valid chunks after processing all files`);
        }

        // âœ… Step 6: æ¸…é™¤æ„å»ºè¿›åº¦ï¼ˆå®Œæˆï¼‰
        storage.clearBuildProgress(diaryName);
        
        console.log(`[VectorDB][Worker] âœ… Full rebuild completed successfully for "${diaryName}" (${Object.keys(chunkMap).length} chunks from ${relevantFiles.length} files).`);
        storage.close();
        return fileHashes;
        
    } catch (error) {
        console.error(`[VectorDB][Worker] âŒ Build failed for "${diaryName}":`, error.message);
        
        // âœ… å…³é”®ï¼šå¤±è´¥æ—¶ä¸åˆ é™¤å·²ä¿å­˜çš„è¿›åº¦ï¼
        // ä¿ç•™ç´¢å¼•æ–‡ä»¶å’Œæ•°æ®åº“æ•°æ®ï¼Œä¸‹æ¬¡å¯ä»¥ç»§ç»­
        try {
            storage.close();
        } catch (e) { /* ignore */ }
        
        throw error;
    }
}

module.exports = { VectorDBManager, processSingleDiaryBookInWorker };